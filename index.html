<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/html">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <link rel="stylesheet" href="docs/katex/katex.min.css">

    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script defer src="docs/katex/katex.min.js"></script>

    <!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="docs/katex/contrib/auto-render.min.js"></script>

    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
              // customised options
              // • auto-render specific keys, e.g.:
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '$', right: '$', display: false},
              ],
              // • rendering keys, e.g.:
              throwOnError : false
            });
        });
    </script>

    <title>Exploring the design space of binary search trees</title>
    <style>

        * {
            padding: 0;
            margin: 0;
        }

        html {
            font-size: 1.2em;
        }

        /**
         * FONTS
         */
        h1  { font-size: 1.6rem }
        h2  { font-size: 2.0rem }
        h3  { font-size: 1.2rem }
        h4  { font-size: 1.0rem }
        p   { font-size: 1.0rem }
        a   { font-size: 1.0rem }
        pre { font-size: 0.8rem }

        .katex { font-size: 1em; padding: 0 0 0 2px; display: inline-block}
        .katex-display {
            display: block;
            padding: 0;
            margin: 2em 0;
        }

        h2 span {
            display: block !important;
            font-size: 1.0rem;
            text-transform: uppercase;
            font-weight: normal;
        }

        /* TITLE */
        #title {
            /*text-align: center;*/
        }

        h2 {
            /*padding: 32px 0;*/
            text-align: center;
        }

        h3 {
            text-align: center;
            text-transform: uppercase;
        }

        #author {
            font-style: italic;
        }

        p  { margin:   0    0 1.0rem 0 }
        h1 { margin: 2.0rem 0 0.5rem 0 }
        h2 { margin: 4.0rem 0 2.0rem 0 }
        h3 { margin: 5.0rem 0 1.0rem 0 }
        h4 { margin: 1.0rem 0 0.5rem 0 }


        a {
            color: #000;
        }

        article {
            font-family: "Charter", "Bitstream Charter", "Cambria", "Noto Serif", serif;
            display: block;
            margin: auto;
            max-width: 38rem;
            line-height: 1.5;
            padding: 0 1rem;
        }
        a, .underline {
            display: inline-block;
            text-decoration: underline;
            text-underline-offset: 2px;
            text-decoration-thickness: 2px;
        }
        article p > span {
            white-space: nowrap;
        }
        cite {
            display: inline-block;
            font-style: normal;
        }
        cite a {
            text-decoration: none;
        }
        cite:before {
            /*content: "["*/
        }
        cite:after {
            /*content: "]"*/
        }
        @media print {
            article a {
                text-decoration: none;
            }
        }
        hr {
            margin: 2rem 0;
            opacity: 0.25;
        }

        section {
            margin: 2rem 0 2rem 0;
        }

        ul {
            margin-left: 2rem;
            margin-bottom: 1rem;
        }

        pre {
            margin: 2rem 0 1rem 2rem;
            font-family: ui-monospace, SFMono-Regular, "SF Mono", Menlo, Inconsolata, Consolas, "Liberation Mono", monospace;
        }

        mark {
            border-radius: 3px;
            background-color: rgba(255, 255, 0, 0.15);
            padding: 2px 0;
        }
        mark.red     { background-color: rgba(255, 20, 150, 0.10) }
        mark.yellow  { background-color: rgba(255, 255, 0, 0.15) }
        mark.green   { background-color: rgba(0, 255, 0, 0.10) }
        mark.blue    { background-color: rgba(0, 0, 255, 0.05) }




        span.quote {
            font-style: italic;
        }
        span.quote:before {
            /*content: "“"*/
        }
        span.quote:after {
            /*content: "”"*/
        }



        .definition {
            margin: 1rem 0 2rem 0;
        }

        .definition .underline {
            margin-bottom: 5px;
        }


        .indent {
            white-space: initial;
            display: inline-block;
            margin-left: 2rem;
        }


        .graphs {
            /*gap: 40px 40px;*/
            /*justify-content: space-between;*/
            /*align-items: flex-start;*/
            /*align-content: space-between;*/
            font-size: 0;
            margin-top: 0;
            margin-bottom: 0;
        }

        .graphs img:nth-child(even) {
            padding-left: 1rem;
        }
        .graphs img:nth-child(odd) {
            padding-right: 1rem;
        }

        .graphs img {
            width: 50%;
            margin-top: 2rem;
            margin-bottom: 1rem;
            /*width: 50%; !* Display images side-by-side *!*/
            /*max-width: 100%; !* Limit image width to its intrinsic size *!*/
            /*margin: 20px 0;*/
            /*max-width: 50%;*/
            box-sizing: border-box;
            /*gap: 40px 40px;*/
            /*flex-grow: 0;*/
            /*width: 50%;*/
            /*padding: 40px;*/
            /*flex-shrink: 1;*/
            /*padding: 0 64px;*/
            /*display: inline-block; !* Display images as inline blocks *!*/
            /*box-sizing: border-box; !* Include border and padding in the width calculation *!*/
            /*vertical-align: top; !* Align images to the top *!*/
        }

        .graphs img:only-child {
            width: 100%;
            /*padding: 0 4rem;*/
        }

        /* Media query for screens with a maximum width of 400px */
        @media (max-width: 720px) {
            .graphs img {
                padding: 0;
                /*width: 100%; !* Display images in full-width *!*/
            }
        }
        .rb-red {
            color: red;
            font-weight: bold;
        }
        .rb-black {
            font-weight: bold;
            color: black;
        }


    </style>

<!--    The guarantees we want from a data structure depend on our model of how that data structure will be used.-->
</head>
<body>
<article>

    <h1 id="title">Exploring the design space of binary search trees</h1>

    <p id="author">Rudi Theunissen, June 2023</p>

    <small>
        This project is still a work in progress.
    </small>

    <p></p>

    <section id="abstract">
        <p>
            The study of <em>binary search trees</em> has been an active research topic in computer science for over 60 years, and most courses on data structures still include them, including
            <a href="http://web.stanford.edu/class/cs166/">Stanford</a> and <a href="https://sp23.datastructur.es/">UC Berkeley</a>.
            Over the years, many binary search tree algorithms have been proposed but only a few variants are commonly found in the wild.
        </p>
        <p>
            Some algorithms are both more intuitive and more efficient, but good implementation references are often difficult to find and therefore difficult to teach and apply.
            Students often miss the opportunity to creatively explore and practice algorithm design because they are distracted by seemingly arbitrary rules and acronyms.
        </p>
        <p>
            The goal of this project is to produce an empirical reference that is easy to study and translate, focusing on experimentation and measurement in an attempt to apply and complement published theory.
            Some of the algorithms implemented and tested herein are presented as such for the first time, which includes a handful of original contributions.
        </p>
    </section>

    <section id="introduction">
        <h2><span>Part 1</span>Introduction</h2>

        <section id="memory">
            <h3>Abstract memory</h3>
            <p>

            </p>
            <p>
                A computer program can use random-access memory to keep information in mind while working on a problem, similar to the concept of <a href=" https://www.cne.psychol.cam.ac.uk/introduction-to-working-memory#:~:text=In%20this%20way%2C%20working%20memory,brain's%20long%2Dterm%20memory).">working memory</a> in humans.
                When more memory is needed, a program can ask an <em>allocator</em> to reserve some, and in return receive a unique identifier to access that specific memory.
                Memory allocated by a program is usually freed by the time the program exits, but during its lifetime there can be many requests to allocate and free individual pieces of memory.
            </p>
            <p>
                We can think of memory conceptually as a collection of documents stored in a massive, physical, indexed filing cabinet.
                To allocate memory, we can ask a cabinet clerk to reserve some space in the cabinet for us, enough for some known or estimated number of documents that we intend to work with.
                Assuming the cabinet is not full, we are provided with an index which uniquely identifies that reserved space in the cabinet.
                Later on when we are done with the work, we let the clerk know that the space we reserved is now free for someone else to use and whatever is stored there can be discarded.
            </p>
            <p>
                A cabinet clerk would quickly become overwhelmed and unable to handle requests without a strategy to organize all the documents.
                There could be many cabinets that collectively store a single collection, and they could be spread across different rooms and offices.
                There could be many clerks working together on the same collection and within the same cabinet.
                The fundamental interactions are simply to allocate, store, retrieve and forget individual memories as needed.
            </p>
            <p>
                What does a program actually <em>do</em> with memory? What is meant by "interaction" exactly?
            </p>
            <p>
                Consider an example problem: to find the average grade of all students in a given school.
                Student records are stored in a database, which includes information such as the name of the student, their registration status, and grade history.
                An existing program starts by allocating enough memory to hold all the records, then loads them from the database into memory, then steps through them one at a time.
                As the program steps through each record, it tracks the sum of all grades to later divide by the number of students to get the average grade.
                With this strategy, the amount of allocation is linearly proportional to the number of students: if the number of students were to double, so would the memory requirement.
            </p>
            <p>
                Another approach might be to not be so eager in the way we read the student records into memory.
                Instead of loading <em>all</em> the students into memory at the start of the program, we can ask the database for one record at a time and only allocate enough memory for one record.
                We can overwrite the current student record in memory with the next student from the database as soon as we add their grades to the sum because the current record is then no longer needed by the algorithm.
                This algorithm therefore requires only a <em>constant</em> amount of memory but would still be linear in time because every student record must be read at least once at some point.
                In this case, if the number of students were to double, the algorithm would take about twice as long to run but would not require any more memory than before.
            </p>
            <p>
                Now consider that a student record also includes a designated friend who is also a student, and that student would then also have a friend, and so forth.
                This creates a recursive relationship because the data model of a student is self-referencing.
                To avoid an infinitely-embedded record structure, the friend field must be stored as a student <em>reference</em>, which a program must first <em>dereference</em> to access the full student record of the friend.
            </p>
            <p>
                The general problem we are trying to solve concerns <em>the energy cost of memory interaction</em>.
                In the case of the cabinet clerks that would be kinetic energy to run around and climb ladders, and in the case of a computer we require electrical energy to modulate the state of transistors.
                A program is considered efficient when it meets its functional requirements using only a small amount of energy relative to the complexity of the problem.
                Some problems can not avoid frequent interaction with memory, but an inefficient algorithm wastes energy by interacting with memory more often than it needs to.
            <p>
                The task at hand is to design data structures in abstract memory that programs can use to organize information in a general way.
                By analogy, we are designing coordinated strategies for the cabinet clerks to keep their documents organized.
            </p>
        </section>

        <section id="linear-lists">
            <h3>Linear lists</h3>
            <p>
                This project explores specifically the implementation of linear lists, which are discrete and dynamic ordered sequences.
                Sequences are a fundamental and ubiquitous data type in algorithm design — search results, events occurring over time, DNA, etc.
                This article itself is a sequence of characters that form a document which might be part of a list of posts somewhere.
                There is no conceptual limit to the size of a sequence, as long as there is enough memory available to store it or space to render it, depending on the context.
            </p>
            <p>
                The following operations describe the list data type used in this project:
            </p>
            <ul>
                <li><strong>Select</strong> reads the value at a given position.</li>
                <li><strong>Update</strong> updates a value at a given position.</li>
                <li><strong>Insert</strong> adds a new value at a given position.</li>
                <li><strong>Delete</strong> removes a value at a given position.</li>
                <li><strong>Join</strong> combines two sequences as one.</li>
                <li><strong>Split</strong> separates a sequence in two.</li>
            </ul>
            <p>
                For a data structure to qualify as a linear list it must support the behaviors that describe the list data type.
                A program using a structure as a list might not know exactly how the structure works internally, only that its abstract behavior is that of a list.
            </p>

            <section id="dynamic-arrays">
                <h4>Dynamic arrays</h4>
                <p>
                    Perhaps the most common computer memory implementation of lists in practice is the <a href="https://en.wikipedia.org/wiki/Dynamic_array">dynamic array</a>.
                    An array is a block of contiguous memory allocated all at once and indexed directly by offset from the address of the block.
                    The total amount of memory allocated by an array is the number of records it can store multiplied by the memory required per record.
                </p>
                <p>
                    To illustrate this, we could use a sheet of grid paper to represent an allocated array where every square is a space in which a single character can be stored.
                    The sheet of grid paper is the array and the squares with characters in them form the sequence.
                    Given that an array is the position of a character is determined by its offset from the start of the array, there can be no spaces between the characters of the sequence within the array.
                    Starting from a blank sheet as an empty list, we might insert one character at a time at the end of the sequence: from the top left corner across the page by column and down the page by row as we start filling up the memory of the array.
                    However, we could also start in the bottom right and spiral inward towards the center.
                </p>
                <p>
                    We are free to organize the memory however we like as long as we continue to maintain the behavior of a list.
                    The sequence of characters we are recording on the grid paper has semantic meaning to the program asking us to record them, but the structure itself is not aware of that.
                    The structure might not know what the data means, and the program might not know how exactly the structure operates.
                </p>
                <p>
                    Data structures usually offer theoretical bounds for every operation to indicate how long an operation might take or how much memory it might allocate, usually proportional to some parameter such as the number of records in the collection.
                    Some data structures are particularly efficient for some operations but less so for others.
                </p>
                <p>
                    Arrays are great at accessing a value by sequential position because the memory address can be calculated as the address of the array plus the search position.
                    Inserting a value at the end of the sequence is very efficient also: simply write into the next empty gap and increment the length of the sequence.
                    However, consider what happens when a value is to be inserted somewhere <em>within</em> the sequence.
                    Since there are no empty spaces, space must first be created by moving every value from that position one step towards the end of the sequence, which is <em>linear</em> in the size of the list.
                    Delete is similarly inefficient, as well as split and join.
                </p>
                <p>
                    What makes an array <em>dynamic</em> is the ability to increase its memory capacity by reallocating the entire sequence into a larger block of memory, copying every value from the original array to the new allocation.
                    Doing so is <em>linear</em> in the size of the sequence but occurs infrequently enough to not be considered a major concern.
                    Common ways to mitigate this cost is to predict an accurate upper-bound as the initial capacity to avoid reallocation entirely, or to otherwise double the previous length when capacity runs out.
                    Insertion, expected to be efficient at the end of the list, is said to be <a href="https://en.wikipedia.org/wiki/Amortized_analysis">amortized</a> because many low-cost operations make up for infrequent large costs.
                </p>
            </section>

            <section id="linked-lists">
                <h4>Linked lists</h4>
                <p>
                    Found in every textbook on algorithms and data structures are <a href="https://en.wikipedia.org/wiki/Linked_list">linked lists</a>, which consist of <em>nodes</em> that each contain a value and a pointer to the next node in the list.
                    The data structure itself keeps a pointer to the first node of the sequence, here referred to as the "root" node, as well as a counter of the total number of linked nodes in its sequence to support the <em>size</em> operation.
                    The last node of the list points to nothing because there is no next value after it in the sequence.
                    A pointer to nothing is a <em>nil</em> pointer or <em>nil</em> in some languages.
                    To insert a new value at a position within the sequence, start at the root and follow pointers one node at a time until the number of nodes encountered equals the target position.
                    At this point, we can insert a new value by allocating a new node for it, pointing the current node to that node, and the new node to the previous next node of the current node.
                    Deleting a node is similar: adjust the link from the node pointing to the node to be deleted to the next node after it, thereby removing that node from the sequence of nodes reachable from the root.
                </p>
                <pre>
(a) -> (b) -> (c) -> (d) -> (e) -> nil
                </pre>
                <p>
                    Many of the operations of a linked list require that we follow a number of links equal to the target position and is therefore <em>linear</em> in the size of the list: the number of memory interactions grows at the same rate as the size of the list.
                    The maximum path length is equal to the size of the list.
                    There is no way around this because the only way to reach a specific node is to follow the path that leads to it, starting from the root node since that is the only reference we have in hand at the start of the operation.
                    There is no need to allocate a large amount of memory all at once or to reallocate anything because memory is allocated individually for each node as needed.
                    The trade-off here is that this requires more allocations overall and often results in memory fragmentation: the next node in the sequence might be physically far away from the current node, compared to an array where the next value is always immediately adjacent because the entire sequence is one contiguous block of memory.
                </p>
                <p>
                    To illustrate this, imagine a very long line of people in a field where everyone is pointing to the next person in line after them.
                    No one knows their current position other than the person at the very front and maybe the next few people after them, but only so because they can see and count the number of people before them.
                    To find the person at some given position, someone (the program) would need to count people one at a time from the start of the line, each time following along to where that person is pointing.
                    "What the point of all this pointing is exactly?".
                    Keep in mind that the people in line might not all be standing neatly in order.
                    In fact, the next person in line might be all the way on the other side of the field and the program would need to walk very far to reach them, which requires a lot of energy and therefore takes a long time.
                </p>
                <p>
                    Imagine now that a ticketing agent is trying to give everyone some chance of being close to the front of the stage.
                    Whenever someone arrives to line up they are assigned a random number between zero and the number of people in line, indicating the number of people they are allowed to skip.
                    Walking up to a random person in line would not help because they would not know what their position is exactly, since others may have joined the line ahead of them since they got there.
                    The only way to operate this strategy is to start at the front of the line and count links along the way.
                    Linked lists are simple to understand and program, but their linear complexity and memory cost often makes them non-viable in practice.
                </p>
            </section>
        </section>

        <section id="binary-search-trees">
            <h3>Binary search trees</h3>

            <p>
                Instead of starting the search at the front of the list, what if we started somewhere in the middle?
                Nodes to the right of this median would point to the next node in the sequence as they did before, but nodes to the left of the median would now point in the other direction towards the front of the list.
                Doing so reduces the maximum path length because the program would now need to walk at most about halfway.
                To achieve this, notice that the median node now has <em>two</em> pointers: one going left and another going right.
            </p>
            <p>
                Applying the same strategy <em>recursively</em> to the left and right sides of the median node produces a structure known as a <em>binary search tree</em>.
                The initial median node is at the very top of the tree, becoming the <em>root</em> node.
                Every node now has two links, one to its left subtree and another to its right subtree.
            </p>

            <section id="node-structure">
                <h4>Node structure</h4>
                <p>
                    Some nodes may not have a left or right subtree, suggesting that either or both links could be nil, but the pointers themselves are still there because they are part of the allocated node structure.
                    <span class="indent">A <em>leaf</em> is an empty subtree denoted by a nil reference.</span><br>
                </p>
                <pre>
type Node struct {
   x Data
   s Size
   l *Node
   r *Node
}
                    </pre>
            </section>

            <section id="traversal">
                <h4>Traversal</h4>
                <p>
                    The fundamental rule that every node must follow is that the value of the left node exists somewhere <em>earlier</em> in the sequence, and the value of the right node exists somewhere <em>after</em>.
                    When this rule is valid at every node in the tree, we get a total linear ordering of all values even though the paths branch in two directions every time.
                    The same sequence of the original linked list can therefore still be derived from the tree structure, making use of an <em>inorder tree traversal</em>.
                </p>
                <p>
                    Starting at the root, perform a traversal of the left subtree, then produce the root, then perform a traversal of the right subtree.
                    The first node produced by this algorithm is therefore the left-most node of the tree and the last node is the right-most node.
                    To illustrate this, we can draw a binary search tree on a grid where the horizontal coordinate is the position of the node in the sequence, and the vertical coordinate is the length of the longest path.
                    Dragging a vertical line across the page from left to right will intersect the nodes in sequential order.
                    The only address we have in hand at the start of the traversal is the root, so to get to the left-most node we need to follow along the <em>left spine</em> of the tree first, eventually coming back up to the root before descending to the left-most node of the right subtree, and so forth.
                </p>
            </section>

            <section id="logarithmic-path-length">
                <h4>Path length</h4>
                <p>
                    Every time we partitioned a sublist around its median we halved its maximum path length, and then we halved the maximum path length of each half, and so on until we reached a single node that could not be partitioned any further.
                    The question becomes "how many times can we divide a number by 2 until we get to 1?".
                    This is known as the <a href="https://en.wikipedia.org/wiki/Binary_logarithm">binary logarithm</a>, written as $\log_{2}$.
                </p>
                <p>
                    The binary log is frequently encountered in computer science and information theory because it relates so closely to binary numbers.
                    For example, the number of bits required to represent an integer $n$ in binary is $\left\lfloor \log_{2}(n) \right\rfloor + 1$.
                    The same reasoning applies to decimal numbers, where the number of digits required to represent an integer $n$ is $\left\lfloor \log_{10}(n) \right\rfloor + 1$, or the number of times we can divide by 10 until we get to 1.
                    A binary number shortens by 1 bit when we divide by 2 and a decimal number shortens by 1 digit when we divide by 10.
                </p>
<!--                <p>-->
<!--                    A two-way branch at every node ideally halves the path length every time and is therefore <em>logarithmic</em> in the size of the list: when the size of the list doubles, the search cost only increases by one.-->
<!--                    By extension, search cost grows linearly when the list grows exponentially.-->
<!--                </p>-->
<!--                <p>-->
<!--                    The maximum path length is the longest path from the root to a node is often referred to as the <span class="quote">height</span> of the tree and can be thought of as the maximum path length.-->
<!--                    The total path length of a binary tree is the sum of the path length from the root to every other node, and the average path length is the total path length divided by the number of nodes.-->
<!--                </p>-->
            </section>

            <section id="insert">
                <h4>Inserting a new node into a binary search tree</h4>
                <p>
                    Recall that inserting a value into in a dynamic array requires the program to first push all values to the right of that position on step forward to create space for the new value.
                    The nil pointers of a binary search tree are exactly these gaps already allocated, where the number of nil pointers is always one more than the number of nodes.
                    There is a space at the front of the sequence (the nil left subtree of the left-most node), between every node, and a space at the end of the sequence (the nil right subtree of the right-most node).
                    Inserting a node into a binary tree replaces one of these nil pointers with a new node, thereby occupying one of the existing spaces but also creating two more.
                </p>
            </section>

            <section id="delete">
                <h4>Deleting a new node from a binary search tree</h4>
                <p>
                    A node to be deleted could exist anywhere in the tree,
                </p>
            </section>

            <section id="relative-position">
                <h4>Relative position</h4>
                <p>
                    We now have a data structure with nodes pointing to other nodes, but it is not yet clear how this operates as a sequence.
                    Traversing a linked list in search of a position is simple because every time we follow a link we count exactly +1 node.
                    To implement search by position on a binary search tree, we need to be able to count towards a position.
                    When we follow a link in either direction along the search path, we skip all the nodes of the opposite branch.
                    At the root of the tree, the first branch skips about half the nodes of the entire sequence, then the second branch skips about a quarter, then an eight, all the way down the tree until we find the node at the position we are searching for.
                </p>
                <p>
                    This is known as <em>binary search</em>, which is a well-known algorithm completely independent of linked nodes and tree structures.
                    Many of us have applied this algorithm in our every-day lives perhaps without realizing it. Consider an old-school dictionary
                    or telephone directory where all the records are printed in alphabetical order. How would you search for a particular entry?
                    If we started on the first page and searched one page at a time it would be a <em>linear search</em>, taking time proportional
                    to the size of the directory. Our intuition might suggest to us that there is a smarter way to go about this, that we can achieve the
                    same result by doing less work: start somewhere in the middle of the book, perhaps not the exact median page but somewhere close to it,
                    then determine whether the page we are looking for is before or after that page. Because the directory is <em>ordered</em>,
                    we know that we can reject either the left or right half entirely. Repeating this step on the resulting half once again
                    divides the search space again in half, until eventually we close in on the page we are looking for.
                </p>
                <p>
                    To implement binary search, we need to determine whether to branch to the left or the right; do we reject the left half or the right half?
                    In the context of a list implementation, this determination requires some <em>comparison by position</em> to either seek further forward or backward in the sequence, relative to the current node.
                    With linked lists, we tracked the current search position by counting every node one at a time, but a binary search tree can skip <em>multiple</em> nodes at a time.
                    To apply a similar counting strategy to binary trees, we need to know the number of nodes in the left subtree: if we were to reject all the nodes of the left subtree, would we skip too far ahead?
                </p>
                <p>
                    We previously structured a binary tree from an existing linked list by choosing the exact median at every step, but binary search does not have such a strict requirement.
                    Even a somewhat poor approximation of the median might only increase the search cost by a few steps.
                    Therefore, we can assert the current node along the search path might not be the exact median of its subtree, and therefore we can not know its position unless we record that information as part of the structure.
                </p>
                <p>
                    How should we track the size of the left subtree?
                </p>
                <p>
                    Perhaps the most common approach to solve this is to store in every node the size of that subtree. [https://en.wikipedia.org/wiki/Order<em>statistic</em>tree] [340 Chapter 14 Augmenting Data Structures CLRS].
                    Inserting a new node requires that we then increment the size field of every node along the search path because the new node that will eventually be
                    attached at the bottom of the tree would be a common descendant, thereby increasing each of their size by 1.
                    To determine the relative position of a node we can dereference its left link to read the size field of that node, wherein lies a fundamental weakness:
                    to know the size of the left subtree we must first dereference it, even though the search path might end up branching to the right.
                    This is a weakness because we have to spend energy to look up the address of a node to know its size.
                </p>
                <p>
                    We could instead store in each node the size of its left subtree specifically, as suggested by Knuth in [] and Crane in [].
                    Keeping a separate <em>size</em> field in the tree alongside the reference to the root node allows us to track the current subtree size at each step along the search path.
                    The size of the right subtree can be calculated as needed as the size of the subtree minus the known size of the left subtree, minus one.
                    This approach allows us to know the subtree sizes of both the left and right nodes without the need to dereference either of them.
                    Insertion then only increments the size field of a node when branching to the left because inserting to the right would
                    not affect the size of the left subtree. This approach therefore reduces memory interaction in exchange for a slightly
                    more complex tree structure and some inherent asymmetry - a good trade.
                </p>
                <p>
                    A third design candidate is to store in each node its position relative its parent, where the parent is the node that points to it.
                    This representation results in a left node having a negative position equal to the negative size of its right subtree minus one, and a right node having a positive position equal to the size of its left subtree plus one.
                    Following a convention where the root node of a tree always has a positive position, the absolute position of any node is then equal to the sum of all relative positions along its path from the root.
                    This strategy is symmetrical, intuitive, and provides one bonus insight: a node is known to be a left or right descendant based on the sign of its position.
                    However, there are two downsides to this representation: the first is that we require one bit to store the sign of the position, thereby halving the utilization of the integer field,
                    and the second is that the resulting algorithms require in some cases additional checks and arguments to indicate and normalize node orientation.
                    Insertion using this strategy would increment the size field when a search path branches left at a right node, and symmetrically decrement the size when branching right at a left node.
                    For example, inserting a node at the very front of the sequence would increment the size of the root node because the size of its left subtree is increasing, then descend along the left spine of the tree without the need to increment any others.
                    Similarly, inserting a node at the very end of the sequence would not increment the size of any node because all the nodes along the right spine including the root have positive positions indicating the size of their left subtrees, unaffected by an insertion to the right.
                </p>
                <p>
                    This last approach is rare but not unheard of.
                    In 2004, Jörg Schmücker <a href="https://markmail.org/message/43ux2i3rbsigtotu?q=TreeList+list:org%2Eapache%2Ecommons%2Edev/&page=4#query:TreeList%20list%3Aorg.apache.commons.dev%2F+page:4+mid:mv2nw4ajw2kywmku+state:results">proposed a list implementation</a> using this exact approach to the Apache Commons Collections library in 2004 [],
                    which is still part of the <a href="https://github.com/apache/commons-collections/blob/3a5c5c2838d0dacbed2722c4f860d36d0c32f325/src/main/java/org/apache/commons/collections4/list/TreeList.java">source</a> at the time of this writing.
                </p>
                <p>
                    In 1997, in section 6.3 of <em>Randomized Binary Search Trees</em>, Martinez and Roura describe the use of an "orientation bit" in every node to indicate which of the two subtrees the size field is referring to.
                    They suggest to flip the orientation at every step along the search path as needed to always store the size of the subtree <em>not</em> increasing in size, thereby leaving behind valid size information in the wake of a failure.
                    Leonor Frias used the bit-orientation technique in [].
                </p>
                <p>
                    This project uses the approach where every node stores the size of its left subtree:
                </p>
                <ul>
                    <li>No need to dereference a node to know its size.</li>
                    <li>No need to consider whether a node is a left or right link.</li>
                    <li>No need to adjust the size field of a node when branching to the right.</li>
                </ul>
                <p>
                    We can now follow this algorithm to search by relative position:
                </p>
                <pre>
func search(p *Node, i uint64) *Node {
   for {
      if i == p.s {
         return p
      }
      if i < p.s {
         p = p.l
      } else {
         i = i - p.s - 1
         p = p.r
      }
   }
}
                </pre>
                <p>
                    When the position we are searching for is equal to the number of nodes in the left subtree, we have found the node we were looking.
                    Skipping all the nodes of the left subtree would skip ahead to exactly this position in the sequence.
                    Otherwise, the position is less than the size of the left subtree, we need to seek towards the front of the list because our current position is still too far ahead in the sequence.
                    In this case, follow the link to the left and continue the search.
                    Otherwise, if the position is greater than the size of the left subtree, we know that we can reject the entire left subtree because even if we skipped all those nodes we would still need to seek further ahead in the sequence, and therefore the node we are looking for must be somewhere in the right subtree.
                    In this case, reduce the search position by the size of the left subtree, including the current node, then descend to the right to continue the search.
                </p>
            </section>
        </section>

        <section id="persistence">
            <h3>Persistence</h3>
            <p>
                A <em>persistent data structure</em> can create many independent versions of itself over time, where changes in a future version would not be visible to a program still referencing an older version.
                Persistence adds the dimension of time as a history, where any version can still be modified to create new versions from that point in time without affecting other versions.
                A program can preserve the current state by first allocating a duplicate structure before making changes, thereby producing a new version entirely.
                For this to be efficient, a data structure must be <em>cheap to copy</em> and implicitly share memory between versions over time.
            </p>

<!--            https://en.wikipedia.org/wiki/Persistent<em>data</em>structure-->

            <section id="reference-counting">
                <h4>Reference counting</h4>
                <p>
                    To avoid copying all the nodes when copying a tree, we allow trees to share common subtrees in memory over time.
                    Some tree far in the future might still point to a node that was allocated in a much earlier version.
                    We need to keep track of these references, so we store in every node a <em>reference count</em> indicating the number of other trees that also reference that node.
                    When the reference count of a node is zero, it suggests that no other trees are aware of that node, so the program can modify it without the need to copy it first.
                    No copying will occur and all modifications will be in-place if a tree never shares a node with another tree.
                    When the reference count of a node is greater than zero, it suggests that another tree has a dependency on it.
                    Before making a change to this node, which at this point would also change the other trees that depend on it, we must first detach it from its history.
                    This can be done by (1) replacing the node by a duplicate of its node structure, (2) incrementing the reference counts of the left and right links, (3) decrementing the reference count of the original node, and (4) setting the reference count of the copy to zero.
                    There is now one fewer tree depending on that node specifically, and because the reference count of the new node is zero, the program can make changes to it as needed.
                </p>

<!--                https://en.wikipedia.org/wiki/Copy-on-write-->
<!--                https://en.wikipedia.org/wiki/Reference_counting-->
<!--                https://en.wikipedia.org/wiki/Immutable_object-->
<!---->
            </section>


            <section id="path-copying">
                <h4>Path copying</h4>
                <p>
                    Consider now that some node is being modified in the depths of some version of a tree, but that node is shared, so must first be copied.
                    How can the root node of the tree know about that new branch?
                    How could it reach it?
                    Looking at it from the other side, a node replaces another node on the search path when it is copied, so there is some parent node now pointing to that copy.
                    The program would need to change the left link of that parent node to point to the copy, so it too must first be copied.
                    This continues until it cascades all the way up to the root of the tree.
                    There is only one rule to consider when thinking about path copying: during an operation, all paths that lead to a modification must be copied.
                    Every node has a unique path from the root, so for any operation we can mark the nodes that would need to be modified and trace their paths back up to the root;
                    it is these paths that would need to be copied so that they all belong to the same, new version of the tree that includes those modifications.
                </p>
<!--                https://en.wikipedia.org/wiki/Persistent<em>data</em>structure#:~:text=in%20the%20array.-,Path%20copying,-%5Bedit%5D-->
            </section>
            <p>
                TODO MIX IN HERE, APPLIES TO ALL:<br>
                An AVL tree is always height-balanced, so the maximum path length is always logarithmic in the size of the tree.
                Recall that a persistent update requires that we copy all paths that lead to a modification.
                Since all structural adjustments on an AVL tree are local to the search path, a persistent update would need to copy all nodes along the search path.
                Persistence is therefore well-supported by AVL trees because a persistent update would only need to copy a logarithmic number of nodes.
            </p>

            <section id="parent-pointers">
                <h4>Parent pointers</h4>
                <p>
                    Many implementations of binary search tree algorithms involve one additional pointer: the <em>parent</em> pointer, pointing back up to the node that points to it.
                    Path copying requires that _during an operation, all paths that lead to a modification must be copied_, but with parent pointers all paths lead to all nodes.
                    Parent pointers create cycles between nodes, which is not compatible with path-copying and therefore not part of the node structure used in this project.
                    Avoiding parent pointers also saves on memory interaction because there are fewer pointer relationships to maintain, and no need to allocate space for the pointer in the node structure.
                </p>
            </section>
        </section>

        <section id="concurrency">
            <h3>Concurrency</h3>
            <p>
                A data structure that supports <em>concurrency</em> provides in some way the ability to run multiple operations at the same time.
                This is not exactly <em>parallelism</em>, which is where the work of a single operation is divided up and worked on concurrently, together towards the same outcome.
                Concurrency, for example, is where an operation to insert a new value does not prevent another similar operation from starting before the previous one ends.
                To illustrate this idea on a binary search tree, imagine two insert operations starting at the same time.
                One of these operations must win the race to the root node, and the other operation must wait in line right behind it: this is called <em>contention</em>.
                The first insert operation must branch to the left of the root, so it increments the size of the left subtree and follows the left link, never to be seen again.
                The second operation is clear to operate on the root node as soon as the first operation follows that left link:
                if it too must branch left then it will likely be blocked by the first operation again, but if the second operation branches to the right then these two operations are forever independent.
                An operation could modify a node in one part of the tree while another operation is modifying a different part of the same tree.
            </p>

            <section id="recursion">
                <h4>Recursion</h4>
                <p>
                    Binary tree algorithms often use <em>recursion</em> to implement operations in two phases: (i) a search phase descending from the root, and (ii) a balancing phase ascending back towards the root.
                    Another concurrent operation would need to wait for the algorithm to go all the way down, then come all the way back up to make final adjustments before having access to the node.
                    For this reason, prefer whenever possible to implement algorithms as one iterative top-down phase, effectively combining the search and balancing phases into one.
                    Some literature use <span class="quote">top-down</span> and <span class="quote">iterative</span> to mean the same thing, but here we define top-down specifically as a single top-down phase in constant space.
                    Recursive implementations are still valuable because they are often simpler to implement and help to verify iterative implementations.
                </p>
            </section>
        </section>
    </section>

    

    <section id="strategies-to-restore-balance">
        <h2><span>Part 2</span>Restoring Balance</h2>
        <p>
            Consider what happens to the structure of the tree if we repeatedly inserted many nodes at the end of the sequence.
            Eventually, the tree structure becomes <em>unbalanced</em>, where too many branches are too far from the median of their sequence.
            Starting from an empty tree, always inserting at the end of the sequence would create precisely a linked list.
            Over time, even a perfectly balanced tree might become unbalanced when nodes are inserted, deleted, split, and joined.
        </p>
        <p>
            The same tree can be arranged in many unique ways[Catalan numbers] without changing the linear order of its nodes.
            There is always a way to arrange a tree so that any of its nodes could be the root.
            For example, using the first node of a sequence as the root would result in a tree with no left subtree.
        </p>
        <pre>
a  b  c         a  b  c           a  b  c

(a)                (b)                  (c)
  ╲               ╱   ╲                /
   (b)          (a)   (c)            (b)
     ╲                              /
      (c)                         (a)

            </pre>

        <p>
            Keep in mind that a node only stores reference to other nodes, not the nodes themselves, so to get the information of another node we must dereference it.
            The program must therefore interact with memory whenever it follows a link from one node to another.
            Reducing the number of links to follow is therefore a strategy to reduce interaction with memory.
        </p>

        <p class="definition"><span class="underline">Definition:</span><br>
            <span class="indent">The <em>path length</em> of a node is the number of links to follow to reach it from the root.</span>
        </p>
        <p class="definition"><span class="underline">Definition:</span><br>
            <span class="indent">The <em>average path length</em> is the sum of all path lengths divided by the size of the tree.</span>
        </p>

        <p>
            Balance is therefore a measure of low average path length: on average, how many links would a program need to follow?
            To restore balance, we need a strategy to reduce the average path length by adjusting the branches of a tree without changing its underlying sequence.
            Some balancing strategies spend a lot of energy to achieve perfect balance, while others spend less energy by being more relaxed.
            We can evaluate these strategies as a trade-off between balance and energy: to what extent can balance be relaxed before search cost becomes too much?
        </p>

        <p class="definition"><span class="underline">Definition:</span><br>
            <span class="indent">A tree is balanced if every node is balanced by some definition of balance.</span>
        </p>


        <section id="rotations">
            <h3>Rotations</h3>
            <p>
                A tree <em>rotation</em> is a minor local branch adjustment that changes the path lengths of some nodes without changing their order.
                Consider the three of nodes in 1 and the transformation required to form the tree in 2:
                move (c) up from the right, pushing (a) down to the left, dragging (d) along with it, and (b) moves across.
                The transformation from 2 to 3 is similar:
                move (d) up from the right, pushing (c) and its left subtree (a) down to the left.
                Rotating back in the other direction at (d) and then (c) revert back to 1.
                At all stages, the sequence was (a,b,c,d).
            </p>
            <pre>

           1                       2                      3

1         (a)                     (c)                    (d)
             ╲                   ╱   ╲                   /
2            (c)               (a)   (d)               (c)
             ╱  ╲                ╲                     /
3          (b)  (d)               (b)                (a)
                                                          ╲
                                                          (b)
    </pre>

            <pre>
func (p *Node) rotateL() *Node {       func (p *Node) rotateR() *Node {
   r := p.r                               l := p.l
   p.r = r.l                              p.l = l.r
   r.l = p                                l.r = p
   r.s = r.s + p.s + 1                    p.s = p.s - l.s - 1
   return r                               return l
}                                      }
            </pre>

            <p>
                A well-known algorithm to restore balance using tree rotations was designed by Quentin Stout and Bette Warren in 1986 [], based on work done by Colin Day in 1976 [].
                Commonly known as the Day-Stout-Warren algorithm or simply DSW, this algorithm first transforms the tree into a linked-list using right-rotations, then transforms that linked-list into a balanced tree using left-rotations.
            </p>

        </section>


        <section id="partitioning">
            <h3>Partitioning</h3>
            <p>
                In 1980, Stephenson [] presented an algorithm that always inserts a new node at the root of the tree by splitting the tree in two: nodes that occur before the new node and nodes that occur after, then setting those two trees as the left and right subtrees of the new node.
                A variation of this algorithm is called <em>partition</em>, which moves the node at a given position to the root of its tree in a single top-down pass:
            </p>

            <pre>
func partition(p *Node, i uint64) *Node {
   n = Node{s: i}
   l = &n
   r = &n
   for i != p.s {
      if i < p.s {
         p.s = p.s - i - 1
         r.l = p
         r = r.l
         p = p.l
      } else {
         i = i - p.s - 1
         l.r = p
         l = l.r
         p = p.r
      }
   }
   r.l = p.r
   l.r = p.l
   p.l = n.r
   p.r = n.l
   p.s = n.s
   return p
}
            </pre>

        </section>

        <section id="median-balance">
            <h3>Median balance</h3>

            <p class="definition"><span class="underline">Definition:</span><br>
                <span class="indent">A node is median-balanced if the size of its left and right subtrees differ by no more than 1.</span>
            </p>
            <p>
                To determine if a node is median-balanced, we can add 1 to the smaller size to see if it becomes greater than or equal to the larger size.
                Without loss of generality, assert that $x \leq y$.
            </p>
            <p>
                $$\text{Median}(x, y) = (x+1) \geq y$$
            </p>
            <p>
                A median-balanced tree is perfectly balanced because every branch divides the search space exactly in half.
                In 2017, Ivo Muusse published an algorithm to balance a binary search tree that uses <em>partition</em> to replace every node of a tree by its underlying median to produce a median-balanced tree <cite data-title="An algorithm for balancing a binary search tree" data-authors="Ivo Muusse" data-year="2017"><a href="docs/references/2017_muusse.pdf"></a></cite>.
            </p>
            <p>
                Partitioning the root around its median distributes its nodes evenly between the left and right subtrees, however there might be nodes within those subtrees that are not median-balanced.
                Repeating this step recursively in the left and right subtrees results in a median-balanced tree.
            </p>

            <pre>
func balance(p *Node, s Size) *Node {
   if p == nil {
      return p
   }
   if not balanced(p) {
      p = tree.partition(p, s / 2)
   }
   p.l = balance(tree, p.l, size(p.l))
   p.r = balance(tree, p.r, size(p.r))
   return p
}
            </pre>


            <p>
                This algorithm has several useful properties:
            </p>

            <ul>
                <li>General for any local definition of balance.</li>
                <li>Operates purely top-down in constant space.</li>
                <li>Subtree balancing is independent so can be done in parallel.</li>
                <li>Efficient when trees are already somewhat balanced.</li>
            </ul>

            <p>
                There are however some node arrangements that are not median-balanced but have the same average path length as if they were median-balanced.
                Consider the following trees that both form the same sequence of 5 nodes with an average path length of 6/5.
                The first tree is not median-balanced because 5/2 is 2, so the median node is (c) but the root is currently (b).
            </p>
            <p>
                The median-balancing strategy would partition at (b) to make (c) the root, but the average path length stays the same.
                This partitioning step is therefore redundant because it did not improve the average or worst-case search cost of the tree.
                Ideally, partitioning by median should only be done if doing so reduces the average path length.
<!--                Ideally, median partitioning should only be done if it would reduce the average path length of the tree.-->

            </p>


            <pre>

    1                            2


   (b)                          (c)
  ╱   ╲                        ╱   ╲
(a)   (d)                    (b)   (d)
     ╱   ╲                   ╱       ╲
   (c)   (e)               (a)       (e)

            </pre>


        </section>


        <section id="height-balance">
            <h3>Height balance</h3>
            <p class="definition">
                <span class="underline">Definition:</span><br>
                <span class="indent">The <em>height</em> of a tree is the maximum path length from its root.</span>
            </p>

            <p class="definition">
                <span class="underline">Definition:</span><br>
                <span class="indent">A node is <em>height-balanced</em> if the height of its left and right subtrees differ by no more than 1.</span>
            </p>

            <p>

                We can continue to use the same partition-based balancing algorithm but change the definition of balance to only partition if the node is not already height-balanced.
                Every node that is not height-balanced is partitioned to become median-balanced, which results in a height-balanced tree.

            </p>

            <p>
                The problem to solve is to determine whether a node is height-balanced.
                Ivo Muusse solves this in <cite data-title="An algorithm for balancing a binary search tree" data-authors="Ivo Muusse" data-year="2017"><a href="docs/references/2017_muusse.pdf"></a></cite> by using a strict definition of height-balance where both the minimum and maximum path lengths of two subtrees differ by no more than 1.
                Then, by pretending that both subtrees are already strictly height-balanced, we can compare their minimum and maximum heights.
<!--                We can then compare their minimum and maximum heights by pretending that both subtrees are already strictly height-balanced.-->

            <p>
                A binary tree is <em>perfect</em> if every node has either zero or two subtrees, thereby forming a complete triangular shape conceptually.
                The height of a perfect binary tree is $\log_{2}(s+1) - 1$, which is intuitive since every level of the tree has twice the number of nodes as the one above it and the number of branches to follow from the root to the bottom of the tree is the number of times that the size can be divided by 2 before reaching 1.
            </p>

            <pre>

                1     3         7             15

                ●     ●         ●              ●
                     ● ●      ●   ●        ●       ●
                             ● ● ● ●     ●   ●   ●   ●
                                        ● ● ● ● ● ● ● ●
            </pre>

            <p>
                When a tree is strictly height-balanced, all the levels are of the tree are full except for maybe the bottom level, where some nodes might have only 1 subtree.
                The bottom level of the smaller subtree is emptied with <em>floor</em>, and the bottom level of the larger subtree is completed with <em>ceil</em>, giving the minimum and maximum heights of the two subtrees.
                We can then determine if the height difference is greater than 1 by comparing these heights.
            </p>
            <p>
                $$\text{Height}(x,y) = \lceil \log_2(\max(x,y)+1) \rceil - \lfloor \log_2(\min(x,y)+1) \rfloor \leq 1$$
            </p>
<!--            <p>-->
<!--                -->
<!--            </p>-->
<!--            <p>-->
<!--                $$\text{Height}(x,y) = \lceil \log_{2}(y+1) \rceil - \lfloor \log_{2}(x+1) \rfloor \leq 1$$-->
<!--            </p>-->
            <p>
                This function, as presented above and by Muusse in [], can be simplified using an identity of the binary logarithm where $\lceil \log_{2}(s+1) \rceil \equiv \lfloor \log_{2}(s) \rfloor + 1$.
                The result is the same whether you complete a level with <em>ceil</em> or empty a level with <em>floor</em> and add one.
                This allows us to express both sides of the inequality in terms of the <em>floor</em> of the binary logarithm.
            </p>
<!--            <p>-->
<!--                Without loss of generality, assert that $x \leq y$:-->
<!--                Given that the number of bits required to represent an integer $n$ in binary is $\lfloor \log_{2}(n) \rfloor + 1$, this becomes a comparison between the number of bits required to represent $x+1$ and $y$.-->
<!--            </p>-->
            <p>
                $$\text{Height}(x,y) = \lfloor \log_{2}(x+1) \rfloor \geq \lfloor \log_{2}(y) \rfloor$$
            </p>

        </section>

        <section id="weight-balance">
            <h3>Weight balance</h3>
<!--            https://yoichihirai.com/bst.pdf-->
<!--            https://i11www.iti.kit.edu/extra/publications/bw-etdwb-20.pdf-->
            <p class="definition">
                <span class="underline">Definition:</span><br>
                <span class="indent">The <em>weight</em> of a tree is the number of leaves reachable from its root, which is one more than the number of nodes in the tree.</span>
            </p>
            <p class="definition">
                <span class="underline">Definition:</span><br>
                <span class="indent">A node is <em>weight-balanced</em> if the weights of its subtrees are within some defined range.</span>
            </p>
            <p>
                The general category of weight-balanced trees require some definition of reasonable difference in the number of nodes between the left and the right subtree.
                Intuitively, there is then an upper-bound on the height of the tree given that every node in the tree is weight-balanced.
                A median-balanced node is therefore also perfectly weight-balanced because there is an ideal distribution in the number of nodes between its and right subtrees, and is therefore also height-balanced.
            </p>
            <p>
                A simple definition of weight-balance is to choose 2 as a constant factor, where a node is balanced if the weight of the smaller subtree is at least half the weight of the larger subtree.
            </p>

            <p>
                $$\text{Weight}(x, y) = (x + 1) \geq (y+1) / 2$$
            </p>
        </section>
        <section id="log-balance">
            <h3>Log balance</h3>
            <p>
                A somewhat overlooked variant of weight-balance was introduced by Salvador Roura in 2001 [], which directly compares the binary logarithm of the weights of the left and right subtrees.
            </p>

            <p class="definition">
                <span class="underline">Definition:</span><br>
                <span class="indent">A node is <em>logarithmically</em> weight-balanced if the discrete binary logarithm of the weights of its subtrees differ by no more than 1.</span>
            </p>

            <p>
                $$
                \begin{aligned}
                \text{Log}(x,y) = &\lfloor\log_{2}(y+1)\rfloor - \lfloor\log_{2}(x+1)\rfloor \leq 1
                \\              = &\lfloor\log_{2}(x+1)\rfloor \geq \lfloor\log_{2}(y+1)\rfloor - 1
                \\              = &\lfloor\log_{2}(x+1)\rfloor \geq \lfloor\log_{2}(y+1)\rfloor - \lfloor\log_{2}(2)\rfloor
                \\              = &\lfloor\log_{2}(x+1)\rfloor \geq \lfloor \log_{2}((y+1)/2) \rfloor
                \end{aligned}
                $$
            </p>
            <p>
                For example, a node with subtree weights 9 and 5 is balanced because $\log_{2}(9) = 4$ and $\log_{2}(5) = 3$ which do not differ by more than 1.
                On the other hand, a node with subtree weights 9 and 3 is <em>not</em> balanced because  $\log_{2}(3) = 2$, which is too far away from 4.
            </p>

            <pre>
                Draw subtrees of 9 and 5 and 9 and 3.
            </pre>

            <p>
                The most-significant-bit or MSB is the position of the leftmost bit set to 1.
                The MSB gives the number of bits required to represent the integer in binary since all the bits to the left of the MSB will be 0 and do not affect the value.
                Comparing the MSB is therefore equivalent to comparing the discrete binary logarithm.
                A node is therefore logarithmically weight-balanced if the MSB of the weight of one subtree is at most one step away from the MSB of the weight of the other subtree.
            </p>

            <pre>
                 BALANCED        NOT BALANCED

                 ↓↓                ↓ ↓
              x: 0<strong>1</strong>01 = 5          00<strong>1</strong>1 = 3
              y: <strong>1</strong>001 = 9          <strong>1</strong>001 = 9

            </pre>
        </section>
        <section id="log-balance">
            <h3>Comparing the binary log</h3>
            <p>
            <p>
                Generally, we require a function that determines whether the MSB of one integer is less than the MSB of another.
                The first candidate is the function used by Roura in [], the second is by Chan in [], and the third is from Warren in [].

                These functions allow us to compare the discrete binary logarithm without the need to calculate the logarithm itself.
            </p>
            <p>
                $$
                \begin{align}
                \lfloor\log_{2}(x)\rfloor \geq \lfloor\log_{2}(y)\rfloor &\equiv x \geq y \text{ || } y \leq (x \mathbin{\&} y) \ll 1
                \\                                                      &\equiv x \geq y \text{ || } x \geq (x \oplus y)
                \\                                                      &\equiv x \geq (\lnot{x} \mathbin{\&} y)
                \end{align}
                $$
            </p>
            <!--
            <p>
                $$
                \begin{align*}
                \lfloor\log_{2}(x)\rfloor \lt \lfloor\log_{2}(y)\rfloor &= x \lt y \text{ and } y \gt (x \mathbin{\&} y) \ll 1
                \\                                                      &= x \lt y \text{ and } x \lt (x \oplus y)
                \\                                                      &= x \lt (\lnot{x} \mathbin{\&} y)
                \end{align*}
                $$
            </p>
            -->
            <p>

            </p>


            <p>
                So far we have four definitions of weight-balance: median-balance (4),  height-balance (5),  weight-balance (6), and logarithmic weight-balance (7).
                There appears to be a pattern where a stronger definition of balance is relaxed by the logarithm — what a neat relationship.

                <!--                There are two defined strategies in terms of the binary logarithm that can be rewritten as bitwise operations.-->

            </p>
            <p>
                $$
                \begin{align*}
                \text{Median}(x, y) &&=&& &(x+1) &\geq& &&y
                \\
                \text{Height}(x,y)  &&=&& \lfloor\log_{2} &(x+1)\rfloor &\geq& &&\lfloor\log_{2}(y)\rfloor
                \\
                \text{Weight}(x, y) &&=&& &(x+1) &\geq& &&(y+1)/2
                \\
                \text{Log}(x, y)    &&=&& \lfloor\log_{2}&(x+1)\rfloor &\geq& &&\lfloor\log_{2}((y+1)/2) \rfloor
                \end{align*}
                $$
            </p>

            <p>
                Using the bitwise function of Warren (3) to replace the binary log comparisons and shifts for division yields the following expanded solutions:

            </p>
            <p>
                $$
                \begin{align*}
                \text{Median}(x, y) &= (x+1) \geq (y)
                \\
                \text{Height}(x,y)  &= (x+1) \geq (y)\mathbin{\&}\lnot{(x+1)}
                \\
                \text{Weight}(x, y) &= (x+1) \geq ((y+1)\gg1)
                \\
                \text{Log}(x, y)    &= (x+1) \geq ((y+1)\gg1)\mathbin{\&}\lnot{(x+1)}
                \end{align*}
                $$
            </p>
<!--            <pre>-->
<!--MSB(x) ≥ MSB(y) = x ≥ y || ((x & y) << 1) ≥ y-->
<!--MSB(x) ≥ MSB(y) = x ≥ y || x ≥ (x ^ y)-->
<!--MSB(x) ≥ MSB(y) = x ≥ (~x & y)-->
<!--            </pre>-->
<!--            MSB(x) ≥ MSB(y) = x ≥ y || ((x & y) << 1) ≥ y    Roura, S.-->
<!--            MSB(x) ≥ MSB(y) = x ≥ y || x ≥ (x ^ y)           Chan, T.M.-->
<!--            MSB(x) ≥ MSB(y) = x ≥ (~x & y)                   Warren, H.S.-->

<!--            <pre>-->
<!--    Median(x, y) =    (<mark class="blue">x + 1) ≥    (<mark class="blue">y)-->
<!--    Height(x, y) = MSB(<mark class="blue">x + 1) ≥ MSB(<mark class="blue">y)-->
<!--  HalfSize(x, y) =    (<mark class="red">x + 1) ≥    (<mark class="red">(y + 1) >> 1)-->
<!--   LogSize(x, y) = MSB(<mark class="red">x + 1) ≥ MSB(<mark class="red">(y + 1) >> 1)-->
<!--            </pre>-->


<!--            <pre>-->
<!--    Height(x, y) = MSB(<mark class="blue">x + 1) ≥ MSB(<mark class="blue">y)-->
<!--    Median(x, y) =    (<mark class="blue">x + 1) ≥    (<mark class="blue">y)-->

<!--   LogSize(x, y) = MSB(<mark class="red">x) ≥ MSB(<mark class="red">y >> 1)-->
<!--  HalfSize(x, y) =    (<mark class="red">x) ≥    (<mark class="red">y >> 1)-->

<!-- LogWeight(x, y) = MSB(<mark class="green">x + 1) ≥ MSB(<mark class="green">(y + 1) >> 1)-->
<!--HalfWeight(x, y) =    (<mark class="green">x + 1) ≥    (<mark class="green">(y + 1) >> 1)-->
<!--            </pre>-->
        </section>
        <section id="log-balance">
            <h3>Cost-optimized balance</h3>
            <p>


            <!--          Roura, S. (2001). A New Method for Balancing Binary Search Trees. I-->
            <!--          Chan, T.M. (2002). Closest-point problems simplified on the RAM. SO-->
            <!--          Warren, H.S. (2002). Hacker's Delight. 2nd Edition, section 5-3.-->

                There is one other design candidate that does not follow the same pattern.
                In 2000, Cho and Sahni introduced the idea of <em>cost-optimized search trees</em>, where the average search cost can not be reduced by a rotation anywhere in the tree.
            <p>
                Even though their definition considers rotations specifically, we can apply this concept to partition-based balancing without the need to consider rotations at all.
                In this context, the only determination to make is whether a node is balanced or not, with no concern for how to restore that balance locally, given that the algorithm uses median-partitioning generally.
                To determine whether a node is cost-optimized by weight, we need to compare two levels of subtree sizes.
            </p>

            <p class="definition">
                <span class="underline">Definition:</span><br>
                <span class="indent">A node is cost-optimized if the size of each subtree is greater than or equal to the size of both subtrees of the other subtree.</span>
            </p>
            <pre>
                                   .
                              l        r
                           ll   lr  rl   rr
            </pre>
            <p>
                $$
                \begin{align*}
                \text{Cost(p)} =  l &\geq rl, l \geq rr, r \geq ll, r \geq lr \\
                \end{align*}
                $$
            </p>
        </section>

        <section id="balancer-analysis">
            <h3>Analysis</h3>
            <p>
                Measurements were made in size increments of 100 up to 1,000,000.
                Random trees were grown for each size and balanced by every strategy.
                The results were generated on an Intel i5 13600K with 32GB of DDR5 memory but should be consistent across platforms, except for duration which is benchmarked separately.
            </p>
            <p>

            </p>

<!--            <div class="graphs">-->
<!--                <img src="benchmarks/svg/balancers/Partition/Uniform/PartitionCount__sbezier.svg">-->
<!--                <img src="benchmarks/svg/balancers/Partition/Uniform/TotalPartitionDepth__sbezier.svg">-->
<!--                <img src="benchmarks/svg/balancers/Partition/Uniform/AveragePartitionDepth__sbezier.svg">-->
<!--                <img src="benchmarks/svg/balancers/Partition/Uniform/Duration__sbezier.svg">-->
<!--                <img src="benchmarks/svg/balancers/Partition/Uniform/AveragePathLength__sbezier.svg">-->
<!--                <img src="benchmarks/svg/balancers/Partition/Uniform/MaximumPathLength__sbezier.svg">-->
<!--            </div>-->
            <section id="partition-count">
                <h4>Partition Count</h4>
                <p>
                    The total partition count is the number of times the balancer needed to partition a node during the balancing of a given random tree at each size increment.
                    Dividing the partition count by the size of the tree at each increment corresponds to the fraction of nodes partitioned.
                    A horizontal line indicates that the number of nodes to partition grows linearly in the size of the tree.
                </p>
                <div class="graphs">
                    <img src="benchmarks/svg/balancers/Partition/Uniform/PartitionCount__sbezier.svg">
                </div>
                <p>
                    The expectation is that height-balance partitions less often than median-balance because it avoids some redundant partitioning, but the results show that height-balance actually partitions more nodes than median-balance.
                    Why is that?
                </p>
                <p>
                    On the other hand, weight-balance partitions less often than all others: $15\%$ of nodes are partitioned for logarithmic weight-balance, $16\%$ for constant weight-balance, and upwards of $30\%$ for height- and median-balance.
                </p>

                <p>
                    Cost-optimized balance is somewhere in-between, partitioning less often than both median-balance and height-balance, but more often than weight-balance.
                    There appears to be a pattern where height-balance follows along the peaks of median-balance and cost-optimized balance follows along the valleys.
                </p>

            </section>

            <section id="partition-depth">
                <h4>Partition Depth</h4>
                <p>
                    The total partition depth is the length of all paths taken by <em>partition</em>, at each size increment.
                    Dividing the total partition depth by the size of the tree gives the average partition depth per node, and plotting partition depth against partition count gives the average partition depth per call.
                </p>


                <div class="graphs">
                    <img src="benchmarks/svg/balancers/Partition/Uniform/TotalPartitionDepth__sbezier.svg">
                </div>
                <p>
                    Interestingly, height-balance partitions more often but takes shorter paths than median-balance during partition.
                    Shorter paths suggests that height-balance usually partitions lower in the tree where subtrees are smaller.
                    This presents a competition: to partition many small subtrees with height-balance, or to partition fewer but larger subtrees with median-balance.
                </p>
                <p>
                    The weight-balance strategies do not partition many nodes but each call to partition is costly in comparison: weight-balance visits $\sim 3.8$ nodes per call to partition compared to median-balance with $\sim 3.4$ and height-balance with $\sim 3.0$.
                </p>
                <p>
                    The partition depth of cost-optimized balance is overall lower than median-balance and height-balance, but not at every size increment.
                    This is a promising result for cost-optimized balance from a structural perspective because it has a low partition count and low partition depth relative to median- and height-balance.
                </p>
                <div class="graphs">
                    <img src="benchmarks/svg/balancers/Partition/Uniform/AveragePartitionDepth__sbezier.svg">
                </div>

            </section>

            <section id="average-path-length">
                <h4>Average Path Length</h4>
                <p>
                    The average path length of the resulting tree measures how well-balanced it is, where a lower average path length indicates a better-balanced tree.
                    Dividing this average by $\log_{2}(n)$ gives the constant factor of the logarithm.
                    As this factor appears to approach $1$, all design candidates produce an average path length of $\sim log_{2}(n)$.
                    For example, the tree of size 1,000,000 at the final increment would have an average path length of about $20$.

                <div class="graphs">
                    <img src="benchmarks/svg/balancers/All/Uniform/AveragePathLength__sbezier.svg">
                </div>
                <p>
                    The results show that median-balance, height-balance and DSW all have optimal average path length, with cost-optimized balance only slightly above optimal.
                    Using weight-balance with a constant factor results in slightly better balance than logarithmic weight-balance, but only $\sim 2\%$.
                </p>
                <p>
                    Notice that the relative difference between all strategies is very small.
                    The average path length at the final increment ranges between $17.9$ and $18.6$, making the percentage difference between the least and most-balanced trees only $\sim 4\%$.
                </p>
            </section>

            <section id="maximum-path-length">
                <h4>Maximum Path Length</h4>
                <p>
                    The maximum path length of the resulting tree measures the worst-case search cost at each size increment, i.e. height.
                    Dividing the path length by $\log_{2}(n)$ gives the constant factor of the logarithm.
                    A horizontal line indicates that the height increases logarithmically in the size of the tree.
                </p>
                <p>
                    The results are the same for the height-balanced strategies because only the lowest level in the tree has missing nodes, which is optimal.
                    As size increases, the lowest level is completed and a new level is started which becomes the longest path.
                    The new level is twice the size of the completed level, so it takes twice as long for the maximum path length to increase again.
                </p>
                <p>
                    There is a $\sim 30\%$ difference in maximum path length between the least and most-balanced trees:
                    cost-optimized balance has a maximum path length of $\sim 1.05\log_{2}$, weight-balance with a constant factor $\sim 1.18\log_{2}$, and logarithmic weight-balance $\sim 1.30\log_{2}$.
                </p>
                <div class="graphs">
                    <img src="benchmarks/svg/balancers/All/Uniform/MaximumPathLength__sbezier.svg">
                </div>
            </section>

            <section id="duration">
                <h4>Duration</h4>
                <p>
                    Trees were also created in size increments of 100,000 up to 10,000,000 to measure the amount of time it takes each strategy to balance random trees on actual hardware.
                    Keep in mind that time-based measurements are system-dependent so the results could be entirely different on other machines.
                </p>
                <div class="graphs">
                    <img src="benchmarks/svg/balancers/All/Uniform/Duration__sbezier.svg">
                </div>
                <p>
                    The results show that median-balance is marginally faster than height-balance, which settles the competition with an unexpected result.
                    Looking at duration cumulatively, it seems that the total duration is practically identical between median-balance, height-balance and cost-optimized balance.
                </p>
                <p>
                    The two weight-balance strategies are equal, both $\sim25\%$ faster than median- and height-balance, and $\sim30\%$ faster than DSW.
                    This suggests that weight-balance with a constant factor is likely the better choice because it takes the same amount of time as logarithmic weight-balance but achieves better balance.
                    However, logarithmic weight-balance requires fewer calls to partition and visits fewer nodes overall, which might offer some benefit on other machines.
                </p>
                <p>
                    Cost-optimized balance has a lower partition count, lower partition depth, and is slightly faster than median-balance and height-balance but slower than weight-balance.
                    The overhead is likely from the memory interaction of the three-node size comparison to determine cost-optimized balance.
                </p>

                <div class="graphs">
                    <img src="benchmarks/svg/balancers/All/Uniform/TotalDuration__cumulative.svg">
                </div>
                <p>
                    DSW appears to be consistently slower to balance large random trees, but the algorithm has some advantages over partition-based balancing that should not be discounted.
                    Both algorithms use constant space, but DSW is $O(n)$ and partition-based balancing is theoretically $O(n\log{n})$[], though could perhaps be proven linear with high probability in the case of random trees.
                </p>
                <p>
                    The two phases of DSW are also useful procedures individually: (i) to convert any binary search tree into a linked list, and (ii) to convert a linked list into a balanced tree, both in linear time and constant space.
                    DSW also does not rely on subtree size which may not be available in some cases.
                </p>
                    Generally, balancing by partition is an excellent strategy when trees are already somewhat-balanced.
                    The weight-balance strategies are the fastest and the resulting balance is only slightly suboptimal, suggesting a good trade in practice when balancing is frequent.
                    Surprisingly, the benchmarks suggest that median-balance is likely the best choice to balance random trees when strict height-balance is a requirement or when balancing is infrequent.
                    DSW is the best choice to balance trees that are likely to be linear lists, or when subtree sizes are not available.
                </p>
            </section>
        </section>
    </section>




    <section id="strategies-to-maintain-balance">
        <h2><span>Part 3</span>Self-Balancing Trees</h2>

        <p>
            Restoring balance to an entire tree is powerful but often disproportionately expensive because the entire tree must be traversed.
            There is also the question of <em>when</em> to balance to avoid balancing too often or to not balance often enough.
            Instead of either balancing the entire tree or not balance at all, we can restore balance incrementally and thereby spread the balancing costs across updates.
        </p>
        <p>
            For example, an algorithm can make structural adjustments along the search path during an update to restore balance locally.
            A valid balancing algorithm does not change the ordering of the nodes and always returns the tree in a balanced state.
            A program can then <em>know</em> that a given tree is balanced because it is always balanced.
        </p>
        <p>
            This section explores various strategies to maintain balance incrementally.
            Some strategies make many structural changes while others are more selective; some maintain strict balance while others are entirely arbitrary.
            The goal is to determine which strategy is likely to be a good choice in a particular situation by considering the nature of balance and the work required to maintain it.
        </p>

        <section id="height-balanced-trees">
            <h3>Rank-balanced trees</h3>
            <p>
                The original self-balancing binary search tree was published in 1962 as <em>an algorithm for the organization of information</em> by <strong>A</strong>delson-<strong>V</strong>elsky and <strong>L</strong>andis.
                <a href="https://en.wikipedia.org/wiki/AVL_tree">AVL trees</a> store the height of every node as part of the node structure, then uses rotations along the search path to maintain height-balance.
            </p>
            <p>
                The most common approach to implement an AVL is tree to use recursive algorithms, where all operations start from the root, traverse along a search path, then backtrack to the root along the search path.
                Another operation on the same tree would need to wait for the current operation to return to the root to potentially adjust it in some way before it can interact with it.
                A recursive algorithm therefore only supports one operation at a time.
            </p>
            <p>
                There is however one variation of the AVL insertion algorithm that provides some support for concurrency.
                In Volume 3 of <em>The Art of Computer Programming</em> [in section 6.2.3], Knuth describes how balancing is only necessary at the topmost node affected by balancing, commonly referred to as the <span><em>safe node</em></span>.
                This node is either the root or the last node along the search path with subtree heights not equal to each other.
                After a new node is inserted at the bottom of the tree, the path from the safe node is traversed again to update node heights, then balanced at the safe node using a subset of the bottom-up deletion balancing algorithm.
                This algorithm is not purely top-down because the search path is traversed multiple times in some cases, but space is constant and everything above the safe node is safe to modify concurrently.
            </p>
            <p>
                Since the invention of AVL trees, many self-balancing strategies have been proposed, the most common being the <a href="https://en.wikipedia.org/wiki/Red%E2%80%93black_tree">red-black tree</a> of Guibas and Sedgewick in 1978.
                Instead of directly storing the height of every node as in AVL trees, red-black trees use a color field in the node structure to track whether a node is <span class="rb-red">red</span> or <span class="rb-black">black</span>.
                The color red was chosen because it was the best-looking color produced by the laser printer available to the authors at the time, and they used red and black pens to draw trees by hand.
                A set of rules based on these colors determine where to make rotations along the search path to maintain balance.
            </p>
            <p>
                Red-black trees are not height-balanced because the heights of two subtrees may differ by more than one in some cases, but the color-based rules do impose a height upper-bound of $2\log_{2}(n)$ where $n$ is the size of the tree.
                The same bound for AVL trees is $\sim 1.44\log_{2}n$, so AVL trees maintain better balance but rotate more often to maintain that balance.
            </p>
            <p>
                Inserting into a red-black tree requires at most two rotations, and deleting at most three.
                Inserting into an AVL also requires at most two rotations, but deletion may require a rotation at every node along the search path.
                The primary advantage of red-black trees is that the balancing overhead per operation is constant.
            </p>
            <p>
                A common response to the recurring question of <em>should I use an AVL tree or a red-black tree</em> is the suggestion to use AVL trees when mostly searching because the balance is better, but to use red-black trees when mostly inserting and deleting because they require fewer rotations.
                This is however a misconception, because inserting and deleting both require a search from the root to the bottom of the tree, which is exactly affected by average path length.
                A red-black tree uses fewer rotations by relaxing the balance, but the relaxed balance results in longer search paths during insert and delete.
                There is a trade-off between the cost of doing more rotations and the cost of traversing longer paths.
            </p>

<!--            <div class="graphs">-->
<!--                <img src="benchmarks/svg/operations/Insert/AVLRedBlack/Duration__sbezier.svg">-->
<!--                <img src="benchmarks/svg/operations/Insert/AVLRedBlack/Rotations__sbezier.svg">-->

<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLRedBlack/Duration__unique.svg">-->
<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLRedBlack/Rotations__unique.svg">-->

<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLRedBlack/AveragePathLength__unique.svg">-->
<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLRedBlack/MaximumPathLength__unique.svg">-->
<!--            </div>
-->

            <p>
                Red-black trees are very common in pedagogy and practice.
                Many of the implementations of the C++ standard library use a red-black tree for sorted sets, as well as Java's <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/TreeMap.html#:~:text=A-,Red%2DBlack%20tree,-based%20NavigableMap%20implementation">TreeMap</a>, and parts of the <a href="https://github.com/torvalds/linux/blob/master/include/linux/rbtree.h">Linux kernel</a>.
                They are generally considered to be a decent all-rounder when it comes to self-balancing binary search trees, but attempting to implement them from reference material often results in frustration.
                Most implementations are recursive and many use parent pointers.
            </p>
            <p>
                In 2013, Haeupler, Sen & Tarjan introduced <a href="https://en.wikipedia.org/wiki/WAVL_tree">rank-balanced trees</a>, which unify many height-based balanced trees under a single framework.
                Rank-balanced trees store in each node an integer rank and use various rank-based rules to define AVL trees, red-black trees, and others.
            </p>
            <p>
                A new strategy emerged from this framework: the <em>weak</em> AVL tree, which effectively combines the insertion of AVL trees with the deletion of red-black trees such that the height bound degrades gracefully from that of an AVL tree as the number of deletions increase, and is never worse than that of a red-black tree.
                Insertion and deletion in a weak AVL tree require at most two rotations, which is fewer than AVL trees and red-black trees.
                Concurrency is well-supported by weak AVL trees because both insertion and deletion can be implemented purely top-down.
            </p>

<!--            <p>-->
<!--                Benchmarks show that weak AVL trees are faster than both AVL trees and red-black trees when only inserting, which is interesting because weak AVL trees have the exact same structure as AVL trees when no deletions occur.-->
<!--                Notice that the number of rotations when inserting is exactly the same between bottom-up AVL trees and bottom-up weak AVL trees.-->
<!--                This suggests that the weak AVL insertion algorithm itself is more efficient, likely due to fewer interactions with memory along the way.-->
<!--                Going through cycles of inserting and deleting show that weak AVL trees are consistently faster than AVL trees and maintain an average path length only slightly higher than AVL trees.-->
<!--                Notice how slowly the average path length worsens compared to red-black trees.-->
<!--            </p>-->


<!--            <div class="graphs">-->
<!--                <img src="benchmarks/svg/operations/Insert/AVLWeakRedBlack/Duration__sbezier.svg">-->
<!--                <img src="benchmarks/svg/operations/Insert/AVLWeakRedBlack/Rotations__sbezier.svg">-->

<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLWeakRedBlack/Duration__sbezier.svg">-->
<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLWeakRedBlack/Rotations__sbezier.svg">-->

<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLWeakRedBlack/AveragePathLength__sbezier.svg">-->
<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLWeakRedBlack/MaximumPathLength__sbezier.svg">-->
<!--            </div>-->

<!--            -->
<!--            <div class="graphs">-->
<!--                <img src="benchmarks/svg/operations/Insert/AVLWeak/Duration__sbezier.svg">-->
<!--                <img src="benchmarks/svg/operations/Insert/AVLWeak/Rotations__sbezier.svg">-->

<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLWeak/Duration__sbezier.svg">-->
<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLWeak/Rotations__sbezier.svg">-->

<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLWeak/AveragePathLength__sbezier.svg">-->
<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLWeak/MaximumPathLength__sbezier.svg">-->
<!--            </div>-->


            <p>
                Following on from this work in 2018, Sen, Tarjan & Kim introduced <a href="http://sidsen.azurewebsites.net//papers/ravl-trees-journal.pdf">relaxed rank-balanced trees</a> where balancing is only done after insertion, not deletion.
                The balance of relaxed rank-balanced trees improves as nodes are inserted, and degrades slowly as nodes are deleted.
                A key difference between the relaxed variants and their conventional counterparts is that the height bound is proportional to the number of insertions rather than the size of the tree.
            </p>

<!--            <p>-->
<!--                How do these strategies compare?-->
<!--            </p>-->

<!--            <p>-->
<!--                This project implements 10 different rank-balanced strategies as pairs of bottom-up and top-down variations.-->
<!--                The results are averaged across multiple access distributions to provide an indicator for the overall expected behavior of each strategy.-->
<!--            </p>-->
<!--            <p>-->
<!--                <em>Insert</em> is an operation where a tree is grown by repeatedly inserting new values until the tree reaches the scale of the benchmark.-->
<!--                This operation indicates how the tree grows and is the most common binary search tree benchmark.-->
<!--            </p>-->
<!--            <p>-->
<!--                <em>InsertDeleteCycles</em> is an operation that grows a tree by insertion, then reduces the tree by half using deletion, then inserts back to scale again for a total of 10 cycles.-->
<!--                This operations indicates how balance changes over time as a result of insertions and deletions.-->
<!--            </p>-->
<!--            <p>-->
<!--                We start by comparing the standard AVL and red-black trees, both bottom-up and top-down.-->
<!--            </p>-->

<!--            <div class="graphs">-->
<!--                <img src="benchmarks/svg/operations/Insert/AVLRelaxed/Duration__sbezier.svg">-->
<!--                <img src="benchmarks/svg/operations/Insert/AVLRelaxed/Rotations__sbezier.svg">-->

<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLRelaxed/Duration__sbezier.svg">-->
<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLRelaxed/Rotations__sbezier.svg">-->

<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLRelaxed/AveragePathLength__sbezier.svg">-->
<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLRelaxed/MaximumPathLength__sbezier.svg">-->
<!--            </div>-->
        </section>
        <section id="height-balanced-trees">
            <h3>Height-balanced trees</h3>
            <p>
                On a completely different track, Prokopec and Odersky presented <a href="https://en.wikipedia.org/wiki/Conc-tree_list">conc-tree lists</a> in 2017, where nodes either have two subtrees or no subtrees.
                Nodes with subtrees are used only for routing purposes, and nodes without subtrees contain the values of the sequence.
                Additionally, like in AVL trees, every node stores its height and the height difference between two nodes differ by no more than 1.
            </p>
            <pre>
        <>          <>  =  routing node, no value
      /    \        ()  =  no subtrees, has value
     <>    (c)
    /  \
  (a)  (b)
            </pre>
            <p>
                Conc-trees are simple to implement and do not use rotations or explicit copying.
                Instead, they make use of functional composition and smart constructors, where rather than copying and modifying an existing node it allocates and composes a new node by combining properties of other nodes.
                Conceptually, conc-trees <em>construct</em> the resulting path more than they modify the existing one.
            </p>

<!--            <div class="graphs">-->
<!--                <img src="benchmarks/svg/operations/InsertPersistent/AVLConc/Duration__sbezier.svg">-->

<!--                <img src="benchmarks/svg/operations/InsertDeleteCyclesPersistent/AVLConc/Duration__sbezier.svg">-->

<!--                <img src="benchmarks/svg/operations/InsertDeleteCyclesPersistent/AVLConc/AveragePathLength__sbezier.svg">-->
<!--                <img src="benchmarks/svg/operations/InsertDeleteCyclesPersistent/AVLConc/MaximumPathLength__sbezier.svg">-->
<!--            </div>-->

        </section>

        <section id="weight-balanced-trees">
            <h3>Weight-balanced trees</h3>

            <!--                        Adams [1] gives a functional implementation-->
            <!--            of weight-balanced trees and claims they perform as-->
            <!--            well as red-black trees, however does not provide a-->
            <!--            practical evaluation.-->

            <!--            With weight-balanced trees, a set-->
            <!--            of balancing parameters (see Section 2.1) play a crucial-->
            <!--            role. While Nievergelt and Reingold introduced the-->
            <!--            technique and conjectured its correctness, the balancing-->
            <!--            technique does not work for the whole range of balance-->
            <!--            parameters they state in their paper. Later, Blum-->
            <!--            and Mehlhorn [4] not only point out this incorrectness,-->
            <!--            but also give a rigorous proof for a smaller space of-->
            <!--            the balancing parameters. Hirai and Yamamoto [8]-->
            <!--            use a computer-assisted proof system to discover the-->
            <!--            whole space of feasible balancing parameters.-->

            <!--            While the original proposal-->
            <!--            (although incorrect, as Blum and Mehlhorn have shown)-->
            <!--            by Nievergelt and Reingold was already a top-down-->
            <!--            algorithm, the supplied proof by Blum and Mehlhorn-->
            <!--            only works for a bottom-up rebalancing.-->


            <!--            Later, Lai-->
            <!--            and Wood [10] have provided a top-down rebalancing-->
            <!--            algorithm and shown its correctness.-->

            <!--            To our knowledge, no empirical-->
            <!--            analysis of top-down weight-balanced trees has been-->
            <!--            done yet.-->

            <!--            comprehensive experimental evaluation of top-down as-->
            <!--            well as bottom-up weight-balanced trees and the possible choices for the balancing parameters,-->

            <!--            We gain the insight that-->
            <!--            top-down weight-balanced trees should be preferred over-->
            <!--            bottom-up weight-balanced trees, and most of the time-->
            <!--            they can compete with the performance of red-black-->
            <!--            trees.-->


            <!--            We can also see that the race between-->
            <!--            red-black trees and weight-balanced trees is a toss-up-->


            <p>
                Implementing a linear list using a binary search tree requires that we store the size of each subtree along the search path.
                It seems ideal then to use this information for balancing, rather than heights or ranks stored additionally in each node.
            </p>
            <p>
                In 1972, Nievergelt and Reingold introduced <em>trees of bounded balance</em>, or BB[$\alpha$] trees, where $\alpha$ is a parameter that determines the balance of the tree.
                They proved that BB[$\alpha$] trees can maintain a maximum height of $1+\log_{1/(1-\alpha)}(n+1)$ by keeping subtree weights within a factor of $\alpha$.
            </p>
            <p>
                The original insertion algorithm was purely top-down, but Blum and Mehlhorn proved in 1980 that the valid range provided for $\alpha$ was incorrect.
                The supplied proof, however, only works for bottom-up balancing.
                In 1993, Lai and Wood described top-down algorithms and proved their correctness for $\frac{2}{11} \lt \alpha \leq 1-\frac{\sqrt2}{2}$.
            </p>
            <p>
                The valid range for these parameters is complex, and their evaluation involves floating-point arithmetic or integer multiplication that could overflow.
                Ideally, there exists a strategy that keeps the weights of subtrees within some bound without the need to perform complex calculations.
            </p>
            <p>
                Along this track is the <em>logarithmic binary search trees</em> introduced by Roura in 2001, which use simple bitwise operations to evaluate logarithmic weight-balance.
                The maximum height of a logarithmic weight-balanced tree $2\log_{2}n$, which is the same as red-black trees and the original weight-balanced trees.
                The original bottom-up algorithm uses subtree size as weight, but Roura mentions in the final remarks that it is possible to use the number of leaves as weight, which aligns with the definition of weight used so far.
            </p>
            <p>
                Leonor Frias published top-down logarithmic weight-balanced trees in 2005, where insertion and deletion is done in constant space.
                Using the standard definition of weight rather than subtree size simplifies their top-down algorithms significantly and becomes general with the top-down algorithms of Lai and Wood.
                Their benchmarks indicate that top-down logarithmic weight-balanced trees are competitive with red-black trees.
            </p>
            <p>
                Hirai and Yamamoto published an investigation of weight-balanced trees in 2011, which included a short section on logarithmic weight-balance stating that "for mathematical reliability, logarithmic weight-balanced trees are simpler".
                They measured the original implementation by Roura against other weight-balanced trees and found their performance to be about the same.
            </p>
            <p>
                Barth and Wagner evaluated top-down weight-balanced trees in 2019, but mention logarithmic weight-balance only in the introduction without further evaluation.
                They observe that "to our knowledge, no empirical analysis of top-down weight-balanced trees has been done yet", and contribute detailed benchmarks across a range of balancing parameters.
                They conclude with a suggestion for further study, that "in the future, it would be interesting to determine the space of feasible balancing parameters for top-down weight-balanced trees similar to how Hirai and Yamamoto have done for bottom-up weight-balanced trees."
            </p>
            <p>
                While not a rigorous proof, this project provides analysis to give an idea of this space for top-down weight-balanced trees.
                For combinations of the balancing parameters $\Delta$ and $\Gamma$, both from 0 to 5 in steps of 0.001, trees of size up to 1,000,000 were created using insert, then emptied with delete.
                After every individual operation, weight-balance was checked at every node and marked as valid if the parameters consistently produced a valid tree throughout the experiment.
                To produce the polytope that represents the space if feasible parameters, a dot is placed at every $\Delta$ and $\Gamma$ combination that produced valid trees.
                False-positives are possible but false-negatives are not, because a negative result indicates that at least one concrete counter-example was found.
            </p>
            <p>
                This project implements four weight-balanced trees:
                <em>WBSTTopDown</em> and <em>WBSTBottomUp</em> are weight-balanced trees that use $\Delta = 3$ and $\Gamma = 2$.
                These parameters are the only integral values in the valid range, and Barth and Wagner describe them as "overall fairly performant", notably faster than red-black trees.
                <em>LBSTTopDown</em> is a top-down logarithmic weight-balanced tree that uses the standard definition of weight, and <em>LBSTBottomUp</em> is the original recursive algorithm by Roura that uses subtree size as weight.
                Both top-down and bottom-up algorithms are exactly the same across variants, except for the definition of balance and determination of single or double rotation.

            </p>
            <!--            <p>-->
            <!--                The height of a logarithmic weight-balanced tree is at most $2\log_{2}n$, which is the same as a red-black tree.-->
            <!--                With performance considered equivalent, there appears to be no practical reason then to use parametric weight-balance over logarithmic weight-balance.-->
            <!--                This project includes two implementations of logarithmic weight-balanced trees: <a>LBSTBottomUp</a> and <a>LBSTTopDown</a>.-->
            <!--            </p>-->
            <!--            <p>-->
            <!--                The bottom-up variant is the original recursive implementation by Roura, but uses a slightly simpler bitwise expression to determine balance: <span class="code">x < ^x & y</span> instead of <span class="code">x < y && ((x&y) << 1) < y</span>.-->
            <!--                The top-down variant is based on the general weight-balancing algorithm of Barth & Wagner that uses subtree weight instead of subtree size; simply $size+1$ instead of $size$.-->
            <!--            </p>-->
        </section>


        <section id="scapegoat-trees">
            <h3>Scapegoat trees</h3>
            <p>
                Exploring further we find the idea of <em>partial rebuilding</em> by Overmars in 1983.
                Further development produced the <a href="">general balanced trees</a> of Andersson in 1989 and 1999, followed by the <a href="">scapegoat tree</a> of Galperin and Rivest in 1993.
            </p>
            <p>
                A tree that is strictly weight-balanced, where every node of the tree is weight-balanced, has a known maximum height.
                For example, the maximum height of logarithmic weight-balanced tree is $2\log_{2}n$.
                Therefore, when the height of a tree exceeds this bound there must be at least one node that is <em>not</em> weight-balanced.
                Galperin and Rivest refer to this node as a <span class="quote">scapegoat</span>.

            </p>
            <p>
                The basic idea of partial rebuilding is to search along the path that exceeds the height bound to find an unbalanced node and then rebuild it into a balanced tree, thereby restoring the height bound.
                Intuitively, the height of a tree that was <em>not</em> balanced decreases when it becomes balanced.
                This strategy combines the ideas of weight- and height-balance by allowing some nodes to not be weight-balanced, as long as the height of the tree is within the height bound.
            </p>
            <pre>

[ image of a tree that has a perfect left subtree
of 15 nodes, a root, and 1 node in its right subtree,
showing that the root is obviously not weight-balanced
but that the tree has a low average search cost ]

            </pre>
            <p>
                The original scapegoat tree uses a parameter $\frac{1}{2} \leq \alpha \lt 1$ and defines height to be no greater than $\lfloor\log_{\frac{1}{\alpha}}\rfloor+1$.
                A tree of size $s = sl + sr + 1$ is $\alpha$-weight-balanced if $sl \leq \alpha * s$ and $sr \leq \alpha * s$</span>.
                The insertion algorithm described by Galperin and Rivest is a two-pass recursive algorithm, (i) search by descending from the root and attach a new node, then (ii) ascend bottom-up to find a scapegoat that is not $\alpha$-weight-balanced.
                There must be at least one such node along the search path when the resulting height of the insertion exceeds the height bound, so when that happens, the rebuilds the subtree of a node that is not $\alpha$-weight-balanced to restore the height bound.
                The suggested rebuilding algorithm recursively flattens the tree into a list, then rotates the list back into a perfectly-balanced tree, therefore similar to the Day-Stout-Warren balancing algorithm.
            </p>

            <p>
                There are a few drawbacks to this strategy:
                (i) the recursion goes all the way back to the root even when the insertion did not exceed the height bound,
                (ii) the $\alpha$-parameter imposes an awkward fractional log base, and
                (iii) the rebuilding algorithm does not consider that the subtree is likely already somewhat-balanced given that the height only just exceeded the bound.
            </p>
            <p>
                Recursion can be avoided by looking for a scapegoat top-down along the search path, before it is known what the height will be.
                The scapegoat is then already in-hand when the height bound is exceeded at the bottom of the tree.
                Evaluating weight-balance optimistically is often redundant however, because the result is not needed when the height remains valid.
                This is okay, given that other weight-balanced trees already evaluate balance at every node along the search path anyway.
            </p>
            <p>
                There can be many nodes along the search path that qualify as a scapegoat, offering a choice to use the first one closest to the root, or to use the last one furthest away from the root.
                For a given operation, accepting the first scapegoat closest to the root allows the algorithm to skip further checks for weight-balance, whereas using the last scapegoat furthest from the root must keep checking weight-balance because it is not known whether another will be found along the way.
            </p>
            <p>
                Choosing the scapegoat furthest from the root is a clear choice when using recursion because once you pass by one on the way up to the root there is no guarantee that another will come along.
                For the algorithm to alternatively use the scapegoat closest to the root, it would need to maintain a reference to the last scapegoat found, which is now potentially far down the tree again since we are back at the root.
                Updating the references from the root to this node is then cumbersome.
                The intuition is also that subtrees are smaller around the lower levels of the tree, so one should prefer to rebuild smaller subtrees which require less work.

            </p>
            <p>
                There are however some hints that rebuilding larger subtrees closer to the root likely produces better results.
                Overmars originally suggested in 1983 to use the node closest to the root [], and Galperin and Rivest write that <em>this heuristic performed better than choosing the first weight-unbalanced ancestor to be the scapegoat</em>.
                Even so, all available references and implementations of scapegoat trees use the recursive method and choose the first scapegoat encountered bottom-up, furthest from the root.
            </p>
            <p>
                The matter of the $\alpha$-parameter can be resolved by using logarithmic weight-balance instead.
                The maximum height of a tree where every node is logarithmically weight-balanced is $2\log_{2}n$, so there must be a node along the search path that is not logarithmically weight-balanced when the height exceeds this bound after an insertion.
                The equivalent $\alpha$ for this bound is $\frac{1}{\sqrt{2}}$.
            </p>
            <p>
                Using logarithmic weight-balance, a tree of size $n$ requires a partial rebuild somewhere along the search path when the height $h$ is greater than twice the binary log of the size $n$.
            </p>
            <p>
                $$
                \begin{aligned}
                \text{Exceeds}(h, n)
                \  &\equiv h \gt 2\log_{2}n
                \\ &\equiv n \lt 2^{(h + 1)/2}
                \\ &\equiv n \lt 1 \ll ((h + 1) \gg 1)
                \end{aligned}
                $$
            </p>

            <p>
                The final sharp edge to resolve is the choice of algorithm to rebuild the scapegoat.
                We can assert that the subtree is already somewhat-balanced, and we know that restoring logarithmic weight-balance at every node in that subtree would restore the height bound.
                Restoring balance using a partition-based strategy is therefore a good choice since most nodes should not require partitioning.
            </p>

            <p>
                <span class="quote">What about deletion?</span>
            </p>

            <p>
                Alongside a reference to the root node and the current size of the tree, a scapegoat tree also stores a <em>maximum size</em> field, which is the maximum size that the tree has reached since the root was rebuilt.
                The theory suggests that the entire tree should be rebuilt when the current size becomes less than $\alpha$ times the maximum size, then to reset the maximum size to the current size.
            </p>
            <p>
                Occasionally rebuilding the tree when many deletions have occurred ensures that the height remains logarithmic in the current size of the tree.
                Alternatively, we can use the concept of relaxed balance by Sen, Tarjan and Kim to only balance after an insertion and not after a deletion.
                The intuition is that height does not increase when a node is deleted, and a following insertion will inevitably restore the height bound, possibly even at the root all the same.
                The height is then at most logarithmic in the number of insertions rather than the current size of the tree.
            </p>
            <p>
                The general pattern of deletion is to search for the node to be deleted then replace it by the <em>join</em> of its subtrees.
                Implementing join for scapegoat trees is difficult, because it is not known where the longest paths exist within each joining tree.
                A reasonable approach is to borrow the join algorithm of weight-balanced trees and leave out the final rotation step.
                The node to delete is then replaced by a subtree that is somewhat-balanced as a result of the join, rather than simply deleting the predecessor or successor.
                This results in better balance and performance overall, mostly due to fewer insertions that exceed the height bound.
            </p>
        </section>


        <section id="randomly-balanced-trees">
            <h3>Randomly-balanced trees</h3>

        </section>

        <section id="self-adjusting-trees">
            <h3>Self-adjusting trees</h3>

        </section>

        <section id="join-based-trees">
            <h3>Join-based trees</h3>

        </section>


    </section>

    <section id="references">
        <h2>References</h2>

    </section>

<!--    <pre>-->
<!--        1. Introduction-->
<!--            - Giving semantic meaning to abstract memory-->
<!--            - Linear lists-->
<!--                - List operations-->
<!--                - Dynamic arrays-->
<!--                - Linked lists-->
<!--            - Binary search trees-->
<!--                - Relative position-->
<!--                - Node structure-->
<!--                - Traversal-->
<!--                - Balance-->
<!--                - Rotations-->
<!--            - Persistence-->
<!--                - Structural sharing-->
<!--                - Immutability-->
<!--                - Parent pointers-->
<!--            - Concurrency-->
<!--                - Contention-->
<!--                - Recursion-->
<!--            - Benchmarking-->
<!--                - Access distributions-->
<!--                - Operations-->
<!--        2. Strategies to restore balance-->
<!--            - Balancing by rotation-->
<!--                - Day-Stout-Warren-->
<!--            - Balancing by recursive median partitioning-->
<!--                - Height balance-->
<!--                - Weight balance-->
<!--        3. Strategies to maintain balance-->
<!--            - Join-based implementations-->
<!--            - Rank-balanced trees-->
<!--            - Weight-balanced trees-->
<!--            - Randomly-balanced trees-->
<!--            - Self-adjusting trees-->
<!--            - Height-balanced trees-->
<!--        4. Evaluation-->
<!--            5. Measurements-->
<!--            6. Evaluation criteria-->
<!--            5. Benchmarks-->
<!--            7. Conclusions-->
<!--               Source code-->
<!--            9. Repository-->
<!--            10. Animations-->
<!--            11. Contributing-->
<!--                -. Open problems-->
<!--            - Missing proofs of correctness-->
<!--            - Complexity of balancing by median partitioning-->
<!--            - Similar analysis for set data structures-->
<!--            - Translation to another language, perhaps Rust-->
<!--            - Comparisons to other list data structures-->
<!--              Notes-->
<!--              References-->
<!--        </pre>-->
<!--    </section>-->
</article>
<!--https://read.seas.harvard.edu/~kohler/notes/llrb.html-->
<!--https://web.archive.org/web/20131106123552/http://t-t-travails.blogspot.com/2008/04/left-leaning-red-black-trees-are-hard.html-->
<!--https://web.archive.org/web/*/http://t-t-travails.blogspot.com/2008/04/left-leaning-red-black-trees-are-hard.html-->
<!--https://github.com/zarif98sjs/RedBlackTree-An-Intuitive-Approach-->
<!--https://www.boost.org/doc/libs/1_53_0/doc/html/intrusive/set_multiset.html-->



<script>
    // Get all cite elements
    const citeElements = document.querySelectorAll('cite');

    // Reference block container
    const referencesContainer = document.getElementById('references');

    // Loop through each cite element
    citeElements.forEach((citeElement, index) => {
        // Increment the index by 1 for display
        const referenceNumber = index + 1;

        // Update the citation number in the <a> tag
        const anchorElement = citeElement.querySelector('a');
        anchorElement.innerText = `[${referenceNumber}]` //`[${citeElement.getAttribute('data-authors')}, ${citeElement.getAttribute('data-year')}]`;

        // Create a new reference block
        const referenceBlock = document.createElement('p');
        referenceBlock.textContent = `[${referenceNumber}] ${citeElement.getAttribute('data-authors')}, "${citeElement.getAttribute('data-title')}", ${citeElement.getAttribute('data-year')}.`;

        // Append the reference block to the container
        referencesContainer.appendChild(referenceBlock);
    });
</script>


</body>
</html>


