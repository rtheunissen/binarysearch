<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Exploring the design space of binary search trees</title>
    <style>

        * {
            padding: 0;
            margin: 0;
        }

        html {
            font-size: 16px;
        }

        /**
         * FONTS
         */
        h1  { font-size: 1.6rem }
        h2  { font-size: 2.0rem }
        h3  { font-size: 1.2rem }
        h4  { font-size: 1.0rem }
        p   { font-size: 1.0rem }
        a   { font-size: 1.0rem }
        pre { font-size: 0.8rem }

        h2 span {
            display: block;
            font-size: 1.0rem;
            text-transform: uppercase;
            font-weight: normal;
        }

        /* TITLE */
        #title {
            /*text-align: center;*/
        }

        h2 {
            /*padding: 32px 0;*/
            text-align: center;
        }

        h3 {
            text-align: center;
            text-transform: uppercase;
        }

        #author {
            font-style: italic;
        }

        p  { margin:   0    0 1.0rem 0 }
        h1 { margin: 2.0rem 0 0.5rem 0 }
        h2 { margin: 4.0rem 0 2.0rem 0 }
        h3 { margin: 4.0rem 0 1.0rem 0 }
        h4 { margin: 1.0rem 0 0.5rem 0 }

        article {
            font-family: Charter, 'Bitstream Charter', 'Sitka Text', Cambria, serif; /* Transitional */
            /*font-family: system-ui, sans-serif;*/
        }

        pre, .math {
            font-family: ui-monospace, monospace;
        }

        .math {
            display: inline-block;
        }

        /*pre.code {*/
        /*    background-color: #fafafa;*/
        /*    border-radius: 8px;*/
        /*    padding: 20px;*/
        /*    margin-left: 0;*/
        /*}*/

        a {
            color: #000;
        }

        article {
            display: block;
            margin: auto;
            max-width: 720px;
            line-height: 1.4;
            padding: 0 1rem;
        }

        hr {
            margin: 2rem 0;
            opacity: 0.25;
        }

        section {
            margin: 2rem 0 2rem 0;
        }

        ul {
            margin-left: 2rem;
            margin-bottom: 1rem;
        }

        pre {
            margin: 2rem 0 1rem 2rem;
        }


        mark {
            border-radius: 3px;
            background-color: rgba(255, 255, 0, 0.15);
            padding: 2px 0;
        }
        mark.red     { background-color: rgba(255, 20, 150, 0.10) }
        mark.yellow  { background-color: rgba(255, 255, 0, 0.15) }
        mark.green   { background-color: rgba(0, 255, 0, 0.10) }
        mark.blue    { background-color: rgba(0, 0, 255, 0.05) }


        .underline, a {
            display: inline-block;
            text-decoration: underline;
            text-underline-offset: 2px;
            text-decoration-thickness: 2px;
        }

        span.quote {
            font-style: italic;
        }
        span.quote:before {
            content: "“"
        }
        span.quote:after {
            content: "”"
        }



        .definition {
            margin: 2rem 0;
        }

        .definition .underline {
            margin-bottom: 5px;
        }


        .indent {
            display: inline-block;
            margin-left: 2rem;
        }

        .subscript {
            font-size: 70%;
            padding-left: 1px;
            position: relative;
            top: 0.4rem;
        }


        .graphs {
            /*gap: 40px 40px;*/
            /*justify-content: space-between;*/
            /*align-items: flex-start;*/
            /*align-content: space-between;*/
            font-size: 0;
            margin-top: 0;
            margin-bottom: 0;
        }

        .graphs img:nth-child(even) {
            padding-left: 1rem;
        }
        .graphs img:nth-child(odd) {
            padding-right: 1rem;
        }

        .graphs img {
            width: 50%;
            margin-top: 2rem;
            margin-bottom: 2rem;
            /*width: 50%; !* Display images side-by-side *!*/
            /*max-width: 100%; !* Limit image width to its intrinsic size *!*/
            /*margin: 20px 0;*/
            /*max-width: 50%;*/
            box-sizing: border-box;
            /*gap: 40px 40px;*/
            /*flex-grow: 0;*/
            /*width: 50%;*/
            /*padding: 40px;*/
            /*flex-shrink: 1;*/
            /*padding: 0 64px;*/
            /*display: inline-block; !* Display images as inline blocks *!*/
            /*box-sizing: border-box; !* Include border and padding in the width calculation *!*/
            /*vertical-align: top; !* Align images to the top *!*/
        }

        .graphs img:only-child {
            width: 100%;
            /*padding: 0 4rem;*/
        }

        /* Media query for screens with a maximum width of 400px */
        @media (max-width: 720px) {
            .graphs img {
                padding: 0;
                /*width: 100%; !* Display images in full-width *!*/
            }
        }
        .rb-red {
            color: red;
            font-weight: bold;
        }
        .rb-black {
            font-weight: bold;
            color: black;
        }


    </style>

<!--    The guarantees we want from a data structure depend on our model of how that data structure will be used.-->
</head>
<body>
<article>

    <h1 id="title">Exploring the design space of binary search trees</h1>

    <p id="author">Rudi Theunissen, June 2023</p>

    <small>
        This project is still a work in progress.
    </small>

    <section id="abstract">
        <p>
            The study of <em>binary search trees</em> has been an active research topic in computer science for over 60 years, and most courses on data structures still include them, including
            <a href="http://web.stanford.edu/class/cs166/">Stanford</a> and <a href="https://sp23.datastructur.es/">UC Berkeley</a>.
            Over the years, many binary search tree algorithms have been proposed but only a few variants are commonly found in the wild.
        </p>
        <p>
            Some algorithms are both more intuitive and more efficient, but good implementation references are often difficult to find and therefore difficult to teach and apply.
            Students often miss the opportunity to creatively explore and practice algorithm design because they are distracted by seemingly arbitrary rules and acronyms.
        </p>
        <p>
            The goal of this project is to produce an empirical reference that is easy to study and translate, focusing on experimentation and measurement in an attempt to apply and complement published theory.
            Some of the algorithms implemented and tested herein are presented as such for the first time, which includes a handful of original contributions.
        </p>
    </section>

    

    <section id="introduction">
        <h2><span>Part 1</span>Introduction</h2>

        <section id="memory">
            <h3>Abstract memory</h3>
            <p>

            </p>
            <p>
                A computer program can use random-access memory to keep information in mind while working on a problem, similar to the concept of <a href=" https://www.cne.psychol.cam.ac.uk/introduction-to-working-memory#:~:text=In%20this%20way%2C%20working%20memory,brain's%20long%2Dterm%20memory).">working memory</a> in humans.
                When more memory is needed, a program can ask an <em>allocator</em> to reserve some, and in return receive a unique identifier to access that specific memory.
                Memory allocated by a program is usually freed by the time the program exits, but during its lifetime there can be many requests to allocate and free individual pieces of memory.
            </p>
            <p>
                We can think of memory conceptually as a collection of documents stored in a massive, physical, indexed filing cabinet.
                To allocate memory, we can ask a cabinet clerk to reserve some space in the cabinet for us, enough for some known or estimated number of documents that we intend to work with.
                Assuming the cabinet is not full, we are provided with an index which uniquely identifies that reserved space in the cabinet.
                Later on when we are done with the work, we let the clerk know that the space we reserved is now free for someone else to use and whatever is stored there can be discarded.
            </p>
            <p>
                A cabinet clerk would quickly become overwhelmed and unable to handle requests without a strategy to organize all the documents.
                There could be many cabinets that collectively store a single collection, and they could be spread across different rooms and offices.
                There could be many clerks working together on the same collection and within the same cabinet.
                The fundamental interactions are simply to allocate, store, retrieve and forget individual memories as needed.
            </p>
            <p>
                What does a program actually <em>do</em> with memory? What is meant by "interaction" exactly?
            </p>
            <p>
                Consider an example problem: to find the average grade of all students in a given school.
                Student records are stored in a database, which includes information such as the name of the student, their registration status, and grade history.
                An existing program starts by allocating enough memory to hold all the records, then loads them from the database into memory, then steps through them one at a time.
                As the program steps through each record, it tracks the sum of all grades to later divide by the number of students to get the average grade.
                With this strategy, the amount of allocation is linearly proportional to the number of students: if the number of students were to double, so would the memory requirement.
            </p>
            <p>
                Another approach might be to not be so eager in the way we read the student records into memory.
                Instead of loading <em>all</em> the students into memory at the start of the program, we can ask the database for one record at a time and only allocate enough memory for one record.
                We can overwrite the current student record in memory with the next student from the database as soon as we add their grades to the sum because the current record is then no longer needed by the algorithm.
                This algorithm therefore requires only a <em>constant</em> amount of memory but would still be linear in time because every student record must be read at least once at some point.
                In this case, if the number of students were to double, the algorithm would take about twice as long to run but would not require any more memory than before.
            </p>
            <p>
                Now consider that a student record also includes a designated friend who is also a student, and that student would then also have a friend, and so forth.
                This creates a recursive relationship because the data model of a student is self-referencing.
                To avoid an infinitely-embedded record structure, the friend field must be stored as a student <em>reference</em>, which a program must first <em>dereference</em> to access the full student record of the friend.
            </p>
            <p>
                The general problem we are trying to solve concerns <em>the energy cost of memory interaction</em>.
                In the case of the cabinet clerks that would be kinetic energy to run around and climb ladders, and in the case of a computer we require electrical energy to modulate the state of transistors.
                A program is considered efficient when it meets its functional requirements using only a small amount of energy relative to the complexity of the problem.
                Some problems can not avoid frequent interaction with memory, but an inefficient algorithm wastes energy by interacting with memory more often than it needs to.
            <p>
                The task at hand is to design data structures in abstract memory that programs can use to organize information in a general way.
                By analogy, we are designing coordinated strategies for the cabinet clerks to keep their documents organized.
            </p>
        </section>

        <section id="linear-lists">
            <h3>Linear lists</h3>
            <p>
                This project explores specifically the implementation of linear lists, which are discrete and dynamic ordered sequences.
                Sequences are a fundamental and ubiquitous data type in algorithm design — search results, events occurring over time, DNA, etc.
                This article itself is a sequence of characters that form a document which might be part of a list of posts somewhere.
                There is no conceptual limit to the size of a sequence, as long as there is enough memory available to store it or space to render it, depending on the context.
            </p>
            <p>
                The following operations describe the list data type used in this project:
            </p>
            <ul>
                <li><strong>Select</strong> reads the value at a given position.</li>
                <li><strong>Update</strong> updates a value at a given position.</li>
                <li><strong>Insert</strong> adds a new value at a given position.</li>
                <li><strong>Delete</strong> removes a value at a given position.</li>
                <li><strong>Join</strong> combines two sequences as one.</li>
                <li><strong>Split</strong> separates a sequence in two.</li>
            </ul>
            <p>
                For a data structure to qualify as a linear list it must support the behaviors that describe the list data type.
                A program using a structure as a list might not know exactly how the structure works internally, only that its abstract behavior is that of a list.
            </p>

            <section id="dynamic-arrays">
                <h4>Dynamic arrays</h4>
                <p>
                    Perhaps the most common computer memory implementation of lists in practice is the <a href="https://en.wikipedia.org/wiki/Dynamic_array">dynamic array</a>.
                    An array is a block of contiguous memory allocated all at once and indexed directly by offset from the address of the block.
                    The total amount of memory allocated by an array is the number of records it can store multiplied by the memory required per record.
                </p>
                <p>
                    To illustrate this, we could use a sheet of grid paper to represent an allocated array where every square is a space in which a single character can be stored.
                    The sheet of grid paper is the array and the squares with characters in them form the sequence.
                    Given that an array is the position of a character is determined by its offset from the start of the array, there can be no spaces between the characters of the sequence within the array.
                    Starting from a blank sheet as an empty list, we might insert one character at a time at the end of the sequence: from the top left corner across the page by column and down the page by row as we start filling up the memory of the array.
                    However, we could also start in the bottom right and spiral inward towards the center.
                </p>
                <p>
                    We are free to organize the memory however we like as long as we continue to maintain the behavior of a list.
                    The sequence of characters we are recording on the grid paper has semantic meaning to the program asking us to record them, but the structure itself is not aware of that.
                    The structure might not know what the data means, and the program might not know how exactly the structure operates.
                </p>
                <p>
                    Data structures usually offer theoretical bounds for every operation to indicate how long an operation might take or how much memory it might allocate, usually proportional to some parameter such as the number of records in the collection.
                    Some data structures are particularly efficient for some operations but less so for others.
                </p>
                <p>
                    Arrays are great at accessing a value by sequential position because the memory address can be calculated as the address of the array plus the search position.
                    Inserting a value at the end of the sequence is very efficient also: simply write into the next empty gap and increment the length of the sequence.
                    However, consider what happens when a value is to be inserted somewhere <em>within</em> the sequence.
                    Since there are no empty spaces, space must first be created by moving every value from that position one step towards the end of the sequence, which is <em>linear</em> in the size of the list.
                    Delete is similarly inefficient, as well as split and join.
                </p>
                <p>
                    What makes an array <em>dynamic</em> is the ability to increase its memory capacity by reallocating the entire sequence into a larger block of memory, copying every value from the original array to the new allocation.
                    Doing so is <em>linear</em> in the size of the sequence but occurs infrequently enough to not be considered a major concern.
                    Common ways to mitigate this cost is to predict an accurate upper-bound as the initial capacity to avoid reallocation entirely, or to otherwise double the previous length when capacity runs out.
                    Insertion, expected to be efficient at the end of the list, is said to be <a href="https://en.wikipedia.org/wiki/Amortized_analysis">amortized</a> because many low-cost operations make up for infrequent large costs.
                </p>
            </section>

            <section id="linked-lists">
                <h4>Linked lists</h4>
                <p>
                    Found in every textbook on algorithms and data structures are <a href="https://en.wikipedia.org/wiki/Linked_list">linked lists</a>, which consist of <em>nodes</em> that each contain a value and a pointer to the next node in the list.
                    The data structure itself keeps a pointer to the first node of the sequence, here referred to as the "root" node, as well as a counter of the total number of linked nodes in its sequence to support the <em>size</em> operation.
                    The last node of the list points to nothing because there is no next value after it in the sequence.
                    A pointer to nothing is a <em>nil</em> pointer or <em>nil</em> in some languages.
                    To insert a new value at a position within the sequence, start at the root and follow pointers one node at a time until the number of nodes encountered equals the target position.
                    At this point, we can insert a new value by allocating a new node for it, pointing the current node to that node, and the new node to the previous next node of the current node.
                    Deleting a node is similar: adjust the link from the node pointing to the node to be deleted to the next node after it, thereby removing that node from the sequence of nodes reachable from the root.
                </p>
                <pre>
(a) -> (b) -> (c) -> (d) -> (e) -> nil
                </pre>
                <p>
                    Many of the operations of a linked list require that we follow a number of links equal to the target position and is therefore <em>linear</em> in the size of the list: the number of memory interactions grows at the same rate as the size of the list.
                    The maximum path length is equal to the size of the list.
                    There is no way around this because the only way to reach a specific node is to follow the path that leads to it, starting from the root node since that is the only reference we have in hand at the start of the operation.
                    There is no need to allocate a large amount of memory all at once or to reallocate anything because memory is allocated individually for each node as needed.
                    The trade-off here is that this requires more allocations overall and often results in memory fragmentation: the next node in the sequence might be physically far away from the current node, compared to an array where the next value is always immediately adjacent because the entire sequence is one contiguous block of memory.
                </p>
                <p>
                    To illustrate this, imagine a very long line of people in a field where everyone is pointing to the next person in line after them.
                    No one knows their current position other than the person at the very front and maybe the next few people after them, but only so because they can see and count the number of people before them.
                    To find the person at some given position, someone (the program) would need to count people one at a time from the start of the line, each time following along to where that person is pointing.
                    "What the point of all this pointing is exactly?".
                    Keep in mind that the people in line might not all be standing neatly in order.
                    In fact, the next person in line might be all the way on the other side of the field and the program would need to walk very far to reach them, which requires a lot of energy and therefore takes a long time.
                </p>
                <p>
                    Imagine now that a ticketing agent is trying to give everyone some chance of being close to the front of the stage.
                    Whenever someone arrives to line up they are assigned a random number between zero and the number of people in line, indicating the number of people they are allowed to skip.
                    Walking up to a random person in line would not help because they would not know what their position is exactly, since others may have joined the line ahead of them since they got there.
                    The only way to operate this strategy is to start at the front of the line and count links along the way.
                    Linked lists are simple to understand and program, but their linear complexity and memory cost often makes them non-viable in practice.
                </p>
            </section>
        </section>

        <section id="binary-search-trees">
            <h3>Binary search trees</h3>

            <p>
                Instead of starting the search at the front of the list, what if we started somewhere in the middle?
                Nodes to the right of this median would point to the next node in the sequence as they did before, but nodes to the left of the median would now point in the other direction towards the front of the list.
                Doing so reduces the maximum path length because the program would now need to walk at most about halfway.
                To achieve this, notice that the median node now has <em>two</em> pointers: one going left and another going right.
            </p>
            <p>
                Applying the same strategy <em>recursively</em> to the left and right sides of the median node produces a structure known as a <em>binary search tree</em>.
                The initial median node is at the very top of the tree, becoming the <em>root</em> node.
                Every node now has two links, one to its left subtree and another to its right subtree.
            </p>

            <section id="node-structure">
                <h4>Node structure</h4>
                <p>
                    Some nodes may not have a left or right subtree, suggesting that either or both links could be nil, but the pointers themselves are still there because they are part of the allocated node structure.
                    A node that has neither a left or right subtree is called a <em>leaf</em> node.
                </p>
                <pre>
type Node struct {
   x Data
   s Size
   l *Node
   r *Node
}
                    </pre>
            </section>

            <section id="traversal">
                <h4>Traversal</h4>
                <p>
                    The fundamental rule that every node must follow is that the value of the left node exists somewhere <em>earlier</em> in the sequence, and the value of the right node exists somewhere <em>after</em>.
                    When this rule is valid at every node in the tree, we get a total linear ordering of all values even though the paths branch in two directions every time.
                    The same sequence of the original linked list can therefore still be derived from the tree structure, making use of an <em>inorder tree traversal</em>.
                </p>
                <p>
                    Starting at the root, perform a traversal of the left subtree, then produce the root, then perform a traversal of the right subtree.
                    The first node produced by this algorithm is therefore the left-most node of the tree and the last node is the right-most node.
                    To illustrate this, we can draw a binary search tree on a grid where the horizontal coordinate is the position of the node in the sequence, and the vertical coordinate is the length of the longest path.
                    Dragging a vertical line across the page from left to right will intersect the nodes in sequential order.
                    The only address we have in hand at the start of the traversal is the root, so to get to the left-most node we need to follow along the <em>left spine</em> of the tree first, eventually coming back up to the root before descending to the left-most node of the right subtree, and so forth.
                </p>
            </section>

            <section id="logarithmic-path-length">
                <h4>Path length</h4>
                <p>
                    Every time we partitioned a sublist around its median we halved its maximum path length, and then we halved the maximum path length of each half, and so on until we reached a single node that could not be partitioned any further.
                    The question becomes "how many times can we divide a number by 2 until we get to 1?".
                    This is known as the discrete <a href="https://en.wikipedia.org/wiki/Binary_logarithm">binary logarithm</a>, written as <span class="math">⌊log<span class="subscript">2</span>⌋</span>.
                </p>
                <p>
                    The discrete binary log is frequently encountered in computer science and information theory because it relates so closely to binary numbers.
                    For example, the number of bits required to encode some integer <span class="math">N</span> in binary is <span class="math">⌊log<span class="subscript">2</span>(N)⌋+1</span>.
                    The same reasoning applies to decimal numbers: the number of digits required to represent a decimal integer <span class="math">N</span> is <span class="math">⌊log<span class="subscript">10</span>(N)⌋+1</span>, or the number of times we can divide by 10 before we get to 1.
                </p>
                <p>
                    When we divide a decimal number by 10, the representation shortens by 1 digit.
                    Similarly, when we divide a binary number by 2, the representation shortens by 1 bit.
                    A two-way branch at every node ideally halves the path length every time and is therefore <em>logarithmic</em> in the size of the list: when the size of the list doubles, the search cost only increases by one.
                    By extension, search cost grows linearly when the list grows exponentially.
                </p>
                <p>
                    The <mark>maximum path length</mark> is the longest path from the root to a node is often referred to as the <span class="quote">height</span> of the tree and can be thought of as the maximum search cost.
                    The total path length of a binary tree is the sum of the path length from the root to every other node, and the <mark>average path length</mark> is the total path length divided by the number of nodes.
                </p>
            </section>

            <section id="insert">
                <h4>Inserting a new node into a binary search tree</h4>
                <p>
                    Recall that inserting a value into in a dynamic array requires the program to first push all values to the right of that position on step forward to create space for the new value.
                    The nil pointers of a binary search tree are exactly these gaps already allocated, where the number of nil pointers is always one more than the number of nodes.
                    There is a space at the front of the sequence (the nil left subtree of the left-most node), between every node, and a space at the end of the sequence (the nil right subtree of the right-most node).
                    Inserting a node into a binary tree replaces one of these nil pointers with a new node, thereby occupying one of the existing spaces but also creating two more.
                </p>
            </section>

            <section id="delete">
                <h4>Deleting a new node from a binary search tree</h4>
                <p>
                    A node to be deleted could exist anywhere in the tree,
                </p>
            </section>

            <section id="relative-position">
                <h4>Relative position</h4>
                <p>
                    We now have a data structure with nodes pointing to other nodes, but it is not yet clear how this operates as a sequence.
                    Traversing a linked list in search of a position is simple because every time we follow a link we count exactly +1 node.
                    To implement search by position on a binary search tree, we need to be able to count towards a position.
                    When we follow a link in either direction along the search path, we skip all the nodes of the opposite branch.
                    At the root of the tree, the first branch skips about half the nodes of the entire sequence, then the second branch skips about a quarter, then an eight, all the way down the tree until we find the node at the position we are searching for.
                </p>
                <p>
                    This is known as <em>binary search</em>, which is a well-known algorithm completely independent of linked nodes and tree structures.
                    Many of us have applied this algorithm in our every-day lives perhaps without realizing it. Consider an old-school dictionary
                    or telephone directory where all the records are printed in alphabetical order. How would you search for a particular entry?
                    If we started on the first page and searched one page at a time it would be a <em>linear search</em>, taking time proportional
                    to the size of the directory. Our intuition might suggest to us that there is a smarter way to go about this, that we can achieve the
                    same result by doing less work: start somewhere in the middle of the book, perhaps not the exact median page but somewhere close to it,
                    then determine whether the page we are looking for is before or after that page. Because the directory is <em>ordered</em>,
                    we know that we can reject either the left or right half entirely. Repeating this step on the resulting half once again
                    divides the search space again in half, until eventually we close in on the page we are looking for.
                </p>
                <p>
                    To implement binary search, we need to determine whether to branch to the left or the right; do we reject the left half or the right half?
                    In the context of a list implementation, this determination requires some <em>comparison by position</em> to either seek further forward or backward in the sequence, relative to the current node.
                    With linked lists, we tracked the current search position by counting every node one at a time, but a binary search tree can skip <em>multiple</em> nodes at a time.
                    To apply a similar counting strategy to binary trees, we need to know the number of nodes in the left subtree: if we were to reject all the nodes of the left subtree, would we skip too far ahead?
                </p>
                <p>
                    We previously structured a binary tree from an existing linked list by choosing the exact median at every step, but binary search does not have such a strict requirement.
                    Even a somewhat poor approximation of the median might only increase the search cost by a few steps.
                    Therefore, we can assert the current node along the search path might not be the exact median of its subtree, and therefore we can not know its position unless we record that information as part of the structure.
                </p>
                <p>
                    How should we track the size of the left subtree?
                </p>
                <p>
                    Perhaps the most common approach to solve this is to store in every node the size of that subtree. [https://en.wikipedia.org/wiki/Order<em>statistic</em>tree] [340 Chapter 14 Augmenting Data Structures CLRS].
                    Inserting a new node requires that we then increment the size field of every node along the search path because the new node that will eventually be
                    attached at the bottom of the tree would be a common descendant, thereby increasing each of their size by 1.
                    To determine the relative position of a node we can dereference its left link to read the size field of that node, wherein lies a fundamental weakness:
                    to know the size of the left subtree we must first dereference it, even though the search path might end up branching to the right.
                    This is a weakness because we have to spend energy to look up the address of a node to know its size.
                </p>
                <p>
                    We could instead store in each node the size of its left subtree specifically, as suggested by Knuth in [] and Crane in [].
                    Keeping a separate <em>size</em> field in the tree alongside the reference to the root node allows us to track the current subtree size at each step along the search path.
                    The size of the right subtree can be calculated as needed as the size of the subtree minus the known size of the left subtree, minus one.
                    This approach allows us to know the subtree sizes of both the left and right nodes without the need to dereference either of them.
                    Insertion then only increments the size field of a node when branching to the left because inserting to the right would
                    not affect the size of the left subtree. This approach therefore reduces memory interaction in exchange for a slightly
                    more complex tree structure and some inherent asymmetry - a good trade.
                </p>
                <p>
                    A third design candidate is to store in each node its position relative its parent, where the parent is the node that points to it.
                    This representation results in a left node having a negative position equal to the negative size of its right subtree minus one, and a right node having a positive position equal to the size of its left subtree plus one.
                    Following a convention where the root node of a tree always has a positive position, the absolute position of any node is then equal to the sum of all relative positions along its path from the root.
                    This strategy is symmetrical, intuitive, and provides one bonus insight: a node is known to be a left or right descendant based on the sign of its position.
                    However, there are two downsides to this representation: the first is that we require one bit to store the sign of the position, thereby halving the utilization of the integer field,
                    and the second is that the resulting algorithms require in some cases additional checks and arguments to indicate and normalize node orientation.
                    Insertion using this strategy would increment the size field when a search path branches left at a right node, and symmetrically decrement the size when branching right at a left node.
                    For example, inserting a node at the very front of the sequence would increment the size of the root node because the size of its left subtree is increasing, then descend along the left spine of the tree without the need to increment any others.
                    Similarly, inserting a node at the very end of the sequence would not increment the size of any node because all the nodes along the right spine including the root have positive positions indicating the size of their left subtrees, unaffected by an insertion to the right.
                </p>
                <p>
                    This last approach is rare but not unheard of.
                    In 2004, Jörg Schmücker <a href="https://markmail.org/message/43ux2i3rbsigtotu?q=TreeList+list:org%2Eapache%2Ecommons%2Edev/&page=4#query:TreeList%20list%3Aorg.apache.commons.dev%2F+page:4+mid:mv2nw4ajw2kywmku+state:results">proposed a list implementation</a> using this exact approach to the Apache Commons Collections library in 2004 [],
                    which is still part of the <a href="https://github.com/apache/commons-collections/blob/3a5c5c2838d0dacbed2722c4f860d36d0c32f325/src/main/java/org/apache/commons/collections4/list/TreeList.java">source</a> at the time of this writing.
                </p>
                <p>
                    In 1997, in section 6.3 of <em>Randomized Binary Search Trees</em>, Martinez and Roura describe the use of an "orientation bit" in every node to indicate which of the two subtrees the size field is referring to.
                    They suggest to flip the orientation at every step along the search path as needed to always store the size of the subtree <em>not</em> increasing in size, thereby leaving behind valid size information in the wake of a failure.
                    Leonor Frias used the bit-orientation technique in [].
                </p>
                <p>
                    This project uses the approach where every node stores the size of its left subtree:
                </p>
                <ul>
                    <li>No need to dereference a node to know its size.</li>
                    <li>No need to consider whether a node is a left or right link.</li>
                    <li>No need to adjust the size field of a node when branching to the right.</li>
                </ul>
                <p>
                    We can now follow this algorithm to search by relative position:
                </p>

                <pre>
p : pointer along the search path
i : search position, also distance

func search(p *Node, i uint64) *Node {
   for {
      if i == p.s {
         return p
      }
      if i < p.s {
         p = p.l
      } else {
         i = i - p.s - 1
         p = p.r
      }
   }
}
                </pre>
                <p>
                    When the position we are searching for is equal to the number of nodes in the left subtree, we have found the node we were looking.
                    Skipping all the nodes of the left subtree would skip ahead to exactly this position in the sequence.
                    Otherwise, the position is less than the size of the left subtree, we need to seek towards the front of the list because our current position is still too far ahead in the sequence.
                    In this case, follow the link to the left and continue the search.
                    Otherwise, if the position is greater than the size of the left subtree, we know that we can reject the entire left subtree because even if we skipped all those nodes we would still need to seek further ahead in the sequence, and therefore the node we are looking for must be somewhere in the right subtree.
                    In this case, reduce the search position by the size of the left subtree, including the current node, then descend to the right to continue the search.
                </p>
            </section>
        </section>

        <section id="persistence">
            <h3>Persistence</h3>
            <p>
                A <em>persistent data structure</em> can create many independent versions of itself over time, where changes in a future version would not be visible to a program still referencing an older version.
                Persistence adds the dimension of time as a history, where any version can still be modified to create new versions from that point in time without affecting other versions.
                A program can preserve the current state by first allocating a duplicate structure before making changes, thereby producing a new version entirely.
                For this to be efficient, a data structure must be <em>cheap to copy</em> and implicitly share memory between versions over time.
            </p>

<!--            https://en.wikipedia.org/wiki/Persistent<em>data</em>structure-->

            <section id="reference-counting">
                <h4>Reference counting</h4>
                <p>
                    To avoid copying all the nodes when copying a tree, we allow trees to share common subtrees in memory over time.
                    Some tree far in the future might still point to a node that was allocated in a much earlier version.
                    We need to keep track of these references, so we store in every node a <em>reference count</em> indicating the number of other trees that also reference that node.
                    When the reference count of a node is zero, it suggests that no other trees are aware of that node, so the program can modify it without the need to copy it first.
                    No copying will occur and all modifications will be in-place if a tree never shares a node with another tree.
                    When the reference count of a node is greater than zero, it suggests that another tree has a dependency on it.
                    Before making a change to this node, which at this point would also change the other trees that depend on it, we must first detach it from its history.
                    This can be done by (1) replacing the node by a duplicate of its node structure, (2) incrementing the reference counts of the left and right links, (3) decrementing the reference count of the original node, and (4) setting the reference count of the copy to zero.
                    There is now one fewer tree depending on that node specifically, and because the reference count of the new node is zero, the program can make changes to it as needed.
                </p>

<!--                https://en.wikipedia.org/wiki/Copy-on-write-->
<!--                https://en.wikipedia.org/wiki/Reference_counting-->
<!--                https://en.wikipedia.org/wiki/Immutable_object-->
<!---->
            </section>


            <section id="path-copying">
                <h4>Path copying</h4>
                <p>
                    Consider now that some node is being modified in the depths of some version of a tree, but that node is shared, so must first be copied.
                    How can the root node of the tree know about that new branch?
                    How could it reach it?
                    Looking at it from the other side, a node replaces another node on the search path when it is copied, so there is some parent node now pointing to that copy.
                    The program would need to change the left link of that parent node to point to the copy, so it too must first be copied.
                    This continues until it cascades all the way up to the root of the tree.
                    There is only one rule to consider when thinking about path copying: during an operation, all paths that lead to a modification must be copied.
                    Every node has a unique path from the root, so for any operation we can mark the nodes that would need to be modified and trace their paths back up to the root;
                    it is these paths that would need to be copied so that they all belong to the same, new version of the tree that includes those modifications.
                </p>
<!--                https://en.wikipedia.org/wiki/Persistent<em>data</em>structure#:~:text=in%20the%20array.-,Path%20copying,-%5Bedit%5D-->
            </section>


            <section id="parent-pointers">
                <h4>Parent pointers</h4>
                <p>
                    Many implementations of binary search tree algorithms involve one additional pointer: the <em>parent</em> pointer, pointing back up to the node that points to it.
                    Path copying requires that _during an operation, all paths that lead to a modification must be copied_, but with parent pointers all paths lead to all nodes.
                    Parent pointers create cycles between nodes, which is not compatible with path-copying and therefore not part of the node structure used in this project.
                    Avoiding parent pointers also saves on memory interaction because there are fewer pointer relationships to maintain, and no need to allocate space for the pointer in the node structure.
                </p>
            </section>
        </section>

        <section id="concurrency">
            <h3>Concurrency</h3>
            <p>
                A data structure that supports <em>concurrency</em> provides in some way the ability to run multiple operations at the same time.
                This is not exactly <em>parallelism</em>, which is where the work of a single operation is divided up and worked on concurrently, together towards the same outcome.
                Concurrency, for example, is where an operation to insert a new value does not prevent another similar operation from starting before the previous one ends.
                To illustrate this idea on a binary search tree, imagine two insert operations starting at the same time.
                One of these operations must win the race to the root node, and the other operation must wait in line right behind it: this is called <em>contention</em>.
                The first insert operation must branch to the left of the root, so it increments the size of the left subtree and follows the left link, never to be seen again.
                The second operation is clear to operate on the root node as soon as the first operation follows that left link:
                if it too must branch left then it will likely be blocked by the first operation again, but if the second operation branches to the right then these two operations are forever independent.
                An operation could modify a node in one part of the tree while another operation is modifying a different part of the same tree.
            </p>

            <section id="recursion">
                <h4>Recursion</h4>
                <p>
                    Binary tree algorithms often use <em>recursion</em> to implement operations in two phases: (i) a search phase descending from the root, and (ii) a balancing phase ascending back towards the root.
                    Another concurrent operation would need to wait for the algorithm to go all the way down, then come all the way back up to make final adjustments before having access to the node.
                    For this reason, prefer whenever possible to implement algorithms as one iterative top-down phase, effectively combining the search and balancing phases into one.
                    Some literature use <span class="quote">top-down</span> and <span class="quote">iterative</span> to mean the same thing, but here we define top-down specifically as a single top-down phase in constant space.
                    Recursive implementations are still valuable because they are often simpler to implement and help to verify iterative implementations.
                </p>
            </section>
        </section>
    </section>

    

    <section id="strategies-to-restore-balance">
        <h2><span>Part 2</span>Restoring Balance</h2>
        <p>
            Consider what happens to the structure of the tree if we repeatedly inserted many nodes at the end of the sequence.
            Eventually, the tree structure becomes <em>unbalanced</em>, where too many branches are too far from the median of their sequence.
            Starting from an empty tree, always inserting at the end of the sequence would create precisely a linked list.
            Over time, even a perfectly balanced tree might become unbalanced when nodes are inserted, deleted, split, and joined.
        </p>
        <p>
            The same tree can be arranged in many unique ways[Catalan numbers] without changing the linear order of its nodes.
            There is always a way to arrange a tree so that any of its nodes could be the root.
            For example, using the first node of a sequence as the root would result in a tree with no left subtree.
        </p>
        <pre>
a  b  c         a  b  c           a  b  c

(a)                (b)                  (c)
  ╲               ╱   ╲                /
   (b)          (a)   (c)            (b)
     ╲                              /
      (c)                         (a)

            </pre>

        <p>
            Keep in mind that a node only keeps pointers to other nodes, not the nodes themselves, so to get the information of another node we need first retrieve it from memory.
            Every time the program follows a link from one node to another, it must interact with memory to dereference that next node.
            Generally, a tree search uses less energy when there are fewer links to follow per search.
            Balance is therefore a measure of low <em>average path length</em>: on average, how many links would a program need to follow?
            The path length of a node is the number of links to reach it from the root.
            The average path length is the sum of all path lengths divided by the size of the tree.
        </p>
        <p>
            To restore balance, we need a strategy to reduce the average path length by adjusting the branches of a tree without changing its underlying sequence.
            Some balancing strategies spend a lot of energy to achieve perfect balance, while others spend less energy by being more relaxed.
            We can evaluate these strategies as a trade-off between balance and energy: to what extent can balance be relaxed before search cost becomes too much?
        </p>

        <section id="rotations">
            <h3>Rotations</h3>
            <p>
                A tree <em>rotation</em> is a minor local branch adjustment that changes the path lengths of some nodes without changing their order.
                Consider the three of nodes in 1 and the transformation required to form the tree in 2:
                move (c) up from the right, pushing (a) down to the left, dragging (d) along with it, and (b) moves across.
                The transformation from 2 to 3 is similar:
                move (d) up from the right, pushing (c) and its left subtree (a) down to the left.
                Rotating back in the other direction at (d) and then (c) revert back to 1.
                At all stages, the sequence was (a,b,c,d).
            </p>
            <pre>

           1                       2                      3

1         (a)                     (c)                    (d)
             ╲                   ╱   ╲                   /
2            (c)               (a)   (d)               (c)
             ╱  ╲                ╲                     /
3          (b)  (d)               (b)                (a)
                                                          ╲
                                                          (b)
    </pre>

            <p>
                A well-known algorithm to restore balance using tree rotations was designed by Quentin Stout and Bette Warren in 1986 [1], based on work done by Colin Day in 1976 [], commonly known as the Day-Stout-Warren algorithm or simply DSW.
                This algorithm first transforms the tree into a linked-list using right-rotations, then transforms that linked-list into a balanced tree using left-rotations.
            </p>

        </section>


        <section id="partitioning">
            <h3>Partitioning</h3>
            <p>
                In 1980, Stephenson [] presented an algorithm that always inserts a new node at the root of the tree by splitting the tree in two: nodes that occur before the new node and nodes that occur after, then setting those two trees as the left and right subtrees of the new node.
                A variation of this algorithm is called <em>partition</em>, which moves the node at a given position to the root of its tree in a single top-down pass:
            </p>

            <pre>
func partition(p *Node, i uint64) *Node {
   n := Node{s: i}
   l := &n
   r := &n
   for i != p.s {
      if i < p.s {
         p.s = p.s - i - 1
         r.l = p
         r = r.l
         p = p.l
      } else {
         i = i - p.s - 1
         l.r = p
         l = l.r
         p = p.r
      }
   }
   r.l = p.r
   l.r = p.l
   p.l = n.r
   p.r = n.l
   p.s = n.s
   return p
}
            </pre>

        </section>

        <section id="median-balance">
            <h3>Median balance</h3>

            <p class="definition"><span class="underline">Definition:</span><br>
                <span class="indent">A node is median-balanced if the size of its left and right subtrees differ by no more than 1.</span>
            </p>
            <p class="definition"><span class="underline">Definition:</span><br>
                <span class="indent">A tree is median-balanced if all of its nodes are median-balanced.</span>
            </p>

            <p>
                A median-balanced tree is perfectly balanced because there is no structural arrangement of the same node sequence that has a lower average path length.
                In 2017, Ivo Muusse published an algorithm for balancing a binary search tree [] that uses <em>partition</em> to replace every node of a tree by its underlying median to produce a median-balanced tree.
            </p>

            <p>
                Consider a node of size 10 with a left subtree size of 2 and a right subtree size of 7, which is therefore not median-balanced because the size difference is greater than 1.
                For this node to be median-balanced, there would need to be 4 nodes in one subtree and 5 nodes in the other, either way.
                The median node would be at position 10/2 = 5, so the algorithm moves that node to the top, which in this case pushes the original node down into the left subtree.
                The number of nodes is now evenly distributed between the left and right subtrees, however there might be nodes within those subtrees that are not median-balanced.
                Repeating the same steps recursively in the left and right subtrees results in a median-balanced tree.
            </p>

            <pre>

1. Start with the root of the tree as node P.
2. Partition the median node of P if P is not already balanced.
3. Balance the left subtree of P.
4. Balance the right subtree of P.
            </pre>

            <p>
                This algorithm has multiple useful properties:
            </p>

            <ul>
                <li>General for any definition of local balance.</li>
                <li>Efficient when trees are already somewhat balanced.</li>
                <li>Operates purely top-down in constant space.</li>
                <li>Subtree balancing can be done in parallel.</li>
                <li>Can be cancelled without invalidating the tree.</li>
            </ul>

            <p>
                To determine if a node is median-balanced, we can add 1 to the smaller size to see if it becomes greater than or equal to the larger size.
                Without loss of generality, assume that <span class="math">x < y</span>.
            </p>

            <pre>
<mark class="yellow">Median(x, y) := (x+1) ≥ y</mark>
        </pre>

            <p>
                There are however are some arrangements are not strictly median-balanced but have the same maximum and average path length as a median-balanced tree.
                Consider the trees in [Figure] that both form the same sequence: a,b,c,d,e.
                Both trees have a size of 5, average path length of 6/5, and a maximum path length of 2.
                However, the first tree is not median-balanced because 5/2 is 2 so the median node is (c), but the root is currently (b).
                The median-balancing strategy would partition at (b) to make (c) the root, but the average path length stays the same.
                This partitioning step is therefore redundant because balance did not improve.
            </p>


            <pre>

    1                            2


   (b)                          (c)
  ╱   ╲                        ╱   ╲
(a)   (d)                    (b)   (d)
     ╱   ╲                   ╱       ╲
   (c)   (e)               (a)       (e)

            </pre>


        </section>


        <section id="height-balance">
            <h3>Height balance</h3>

            <p class="definition">
                <span class="underline">Definition:</span><br>
                <span class="indent">The height of a node is equal to its maximum path length.</span>
            </p>

            <p class="definition">
                <span class="underline">Definition:</span><br>
                <span class="indent">The height of a tree is the height of its root.</span>
            </p>

            <p class="definition">
                <span class="underline">Definition:</span><br>
                <span class="indent">A node is height-balanced if the height of its left and right subtrees differ by no more than 1.</span>
            </p>

            <p class="definition">
                <span class="underline">Definition:</span><br>
                <span class="indent">A tree is height-balanced if all of its nodes are height-balanced.</span>
            </p>

            <p>

                We can continue to use the same partition-based balancing algorithm, but change the definition of balance to <mark class="yellow">only partition if the node is not height-balanced</mark>.
                Any node that is not height-balanced is partitioned to become median-balanced, which results in a height-balanced tree.

            </p>

            <p>
                The problem to solve is <mark class="blue">to determine whether a node is height-balanced using only subtree size</mark>.
                Ivo Muusse solves this in [] by using a stricter definition of height-balance where both the minimum and maximum path lengths of two subtrees differ by no more than one.
                Then, by pretending that both subtrees are already strictly height-balanced, we can compare their minimum and maximum heights.
            <p>
                A binary tree is considered <em>perfect</em> when none of the nodes have only one subtree, thereby forming a perfect triangular shape conceptually.
                The height of a perfect binary tree is <span class="math">log<span class="subscript">2</span>(s+1)-1</span>, which is intuitive because it is the number of links you need to follow from the root to get to the bottom of the tree, or conceptually the number of times that you can divide the size of the tree by 2, since every level of the tree has twice the number of nodes as the one above it.
            </p>

            <pre>

        ·               Size    : 7
     ╱     ╲            Height  : 2
   ·        ·
 ╱   ╲    ╱   ╲         log₂(7) ≈ 2.8
·     ·  ·     ·        log₂(8) = 3

            </pre>

            <p>
                When a tree is strictly height-balanced, all the levels are of the tree are full except for maybe the bottom level, where some nodes might have only 1 subtree.
                The bottom level of the larger subtree gets completed with <em>ceil</em>, and the bottom level of the smaller subtree gets emptied with <em>floor</em>.
                We can then determine if the height difference is greater than 1 by comparing these heights:
            </p>
            <pre>
Height(x,y) := ⌈log₂(max(x,y)+1)⌉ - ⌊log₂(min(x,y)+1)⌋ ≤ 1
            </pre>
            <p>
                Without loss of generality, assume that <span class="math">x < y</span>:
            </p>

            <pre>
Height(x,y) := ⌈log₂(y+1)⌉ - ⌊log₂(x+1)⌋ ≤ 1
            </pre>

            <p>
                This function, as presented above and by Muusse in [], <mark class="green">can be simplified using an identity of log₂</mark> where <span class="math">⌈log₂(s+1)⌉ ≡ ⌊log₂(s)⌋+1</span>.
                The result is the same whether you complete the bottom level with <em>ceil</em>, or empty the bottom level with <em>floor</em> and add 1.
            </p>
            <pre>
Height(x,y) := ⌊log₂(y)⌋ - ⌊log₂(x+1)⌋ ≤ 0

Height(x,y) := ⌊log₂(x+1)⌋ ≥ ⌊log₂(y)⌋
            </pre>

            <p>
                Given that the number of bits required to encode an integer <span class="math">i</span> in binary is <span class="math">⌊log₂(i)⌋+1</span>, we can add <span class="math">+1</span> to both sides of the inequality to see that what we are comparing is the number of bits required to encode <span class="math">x+1</span> and <span class="math">y</span>.
                The most-significant bit, or the MSB, is the left-most bit set to 1, starting from position 1 on the right counting left.
                All the bits to the left of the MSB will be 0, so the position of the MSB is equal to the number of bits required to encode the integer.
            </p>
            <p>
                For example:
            </p>
            <pre>
  00001101 = 13, because 8 + 4 + 0 + 1 = 13
      ↑
     MSB at position 4

        log₂(13) ≈ 3.7, so ⌊log₂(13)⌋+1 = 4


 00010001 = 17, because 16 + 0 + 0 + 0 + 1 = 17
    ↑
   MSB at position 5

       log₂(17) ≈ 4.1, so ⌊log₂(17)⌋+1 = 5
       The number of bits required to encode the number 17 in binary is 5
         </pre>
            <p>
                Using this information, we can determine whether a node is height-balanced by comparing the MSB of <span class="math">x+1</span> and <span class="math">y</span>.
                This allows us to determine height-balance without the need to actually calculate either logarithm, which would require slow floating-point calculations to first calculate each logarithm and then floor them.
            </p>

            <pre>
<mark class="yellow">Height(x,y):= MSB(x+1) ≥ MSB(y)</mark>
            </pre>

        </section>

        <section id="weight-balance">
            <h3>Weight balance</h3>
<!--            https://yoichihirai.com/bst.pdf-->
            <p>
                Originally introduced by Nievergelt and Reingold in 1973 as <em>binary search trees of bounded balance</em> [], a node is considered weight-balanced if the size of one subtree is within a constant factor of the other.
                Keeping to the binary theme, we can choose 2 as the constant factor to only partition if the weight of one subtree is more than twice the weight of the other.
            </p>

            <p>
                The classic weight-balance theory uses <em>weight</em> to mean the size + 1, which effectively balances not by subtree size but instead by the number of empty subtrees or nil pointers, which is always size + 1.
                This distinction actually makes a significant difference when balancing by partition.
                For large sizes, a small +1 is insignificant because the weight would need to be very close to the boundary for the +1 to push it into unbalanced territory.
                However, consider what happens in the common case of a subtree of size 3 with height 2:
            </p>
            <pre>
                  TREE                LEFT               RIGHT
   ·              Height : 2          Height : 0         Height : 1
 ╱   ╲            Size   : 3          Size   : 0         Size   : 2
x     ·           Weight : 4          Weight : 1         Weight : 3
    ╱   ╲
   x     ·
       ╱   ╲
      x     x
            </pre>

            <p>
                The empty left subtree has a size of 0 and the right subtree has a size of 2.
                When we define weight as subtree size with a constant balancing factor of 2, this node is <em>not</em> balanced because half of 2 is 1 which is greater than 0.
                In this case, a partition would occur to make the middle node the root and the height would decrease to 1.
                When we instead define weight as subtree size + 1, or equivalently as the number of empty subtrees, we get 1 for the left subtree and 3 for the right which is balanced because 3/2 is 1 using integer division.
                Since this case is very common due to most nodes occurring at the lower levels of the tree, we get a significant reduction in partitioning at the cost of a slightly higher average path length - a good trade.
            </p>

            <p>
                <span class="underline">Definition:</span><br>
                <span class="indent">The weight of a node is the number of underlying empty subtrees or nil pointers, equal to its size + 1.</span>
            </p>

            <p class="definition">
                <span class="underline">Definition:</span><br>
                <span class="indent">A node is weight-balanced if the weight of one subtree is no more than twice the weight of the other.</span>
            </p>

            <p class="definition">
                <span class="underline">Definition:</span><br>
                <span class="indent">A tree is weight-balanced when all of its nodes are weight-balanced.</span>
            </p>

            <pre>
  <mark class="yellow">HalfSize(x, y) := x ≥ y >> 1</mark>

<mark class="yellow">HalfWeight(x, y) := (x + 1) ≥ (y + 1) >> 1</mark>
            </pre>

            <p>
                A somewhat overlooked variant of weight-balance was introduced by Salvador Roura in 2001 [], which directly compares the binary logarithm of the left and right subtree sizes.
                This definition of balance is more relaxed with a maximum height of <span class="math">2*⌊log₂(s)⌋</span>, and fits naturally into the framework of balancing by partition.
                While most of that paper uses subtree size rather than weight, Roura mentions in the final remarks that "it is possible to use the number of [empty subtrees] instead of the number of [nodes] as balancing information".
                This suggestion, alongside the classic weight-balanced trees using size + 1, as well as the fact that log is undefined for 0, suggests that using weight instead of size is likely to yield good results.
            </p>

            <p class="definition">
                <span class="underline">Definition:</span><br>
                <span class="indent">A node is logarithmically weight-balanced if the MSB of the weight of its subtrees differ by no more than 1.</span>
            </p>

            <p>
                For example, a node with subtree sizes 17 and 8 is balanced, but 17 and 7 is <em>not</em> balanced because the most positions of their most-significant bits are too far apart.
            </p>

            <pre>
   ↓↓                    ↓ ↓
00010001 = 17         00010001 = 17
00001000 = 8          00000111 = 7

BALANCED              NOT BALANCED
            </pre>

            <p>
                Assuming <span class="math">x < y</span>, the MSB of <span class="math">y</span> is either at the same position as the MSB of <span class="math">x</span> or further to the left.
                Shifting all the bits of <span class="math">y</span> one step to the right (dividing by 2) then either aligns the MSB of <span class="math">y</span> with the MSB of <span class="math">x</span>, or moves it one step to the right of the MSB of <span class="math">x</span> (if they were already aligned).
                After the shift, if the MSB of <span class="math">y</span> is still to the left of the MSB of <span class="math">x</span> it indicates that the MSB of <span class="math">y</span> was more than one step away and therefore not balanced.
                Balance is therefore met if the MSB of <span class="math">x</span> is greater than or equal to the MSB of <span class="math">y</span> shifted to the right by one step.
                Substituting <span class="math">x+1</span> and <span class="math">y+1</span> yields an expression for logarithmic weight-balance.
            </p>

            <pre>

  <mark class="yellow">LogSize(x, y) := MSB(x) ≥ MSB(y >> 1)</mark>

<mark class="yellow">LogWeight(x, y) := MSB(x + 1) ≥ MSB((y + 1) >> 1)</mark>
            </pre>

            <p>
                We now have 6 definitions of balance in terms of the left and right subtree sizes to balance by partition:
            </p>

            <pre>
    Height(x, y) := MSB(<mark class="blue">x + 1</mark>) ≥ MSB(<mark class="blue">y</mark>)
    Median(x, y) :=    (<mark class="blue">x + 1</mark>) ≥    (<mark class="blue">y</mark>)

   LogSize(x, y) := MSB(<mark class="red">x</mark>) ≥ MSB(<mark class="red">y >> 1</mark>)
  HalfSize(x, y) :=    (<mark class="red">x</mark>) ≥    (<mark class="red">y >> 1</mark>)

 LogWeight(x, y) := MSB(<mark class="green">x + 1</mark>) ≥ MSB(<mark class="green">(y + 1) >> 1</mark>)
HalfWeight(x, y) :=    (<mark class="green">x + 1</mark>) ≥    (<mark class="green">(y + 1) >> 1</mark>)
            </pre>

            <p>
                There appears to be a pattern where a stricter definition of balance is relaxed by using the MSB of the same terms.
                The problem to solve now is how to actually compare the MSB of one integer to the MSB of another.
            </p>
            <p>
                There are a handful of clever ways to do this using bitwise operations [][][]:
            </p>

            <pre>
MSB(x) ≥ MSB(y) := x ≥ y || ((x & y) << 1) ≥ y    Roura, S.
MSB(x) ≥ MSB(y) := x ≥ y || x ≥ (x ^ y)           Chan, T.M.
MSB(x) ≥ MSB(y) := x ≥ (~x & y)                   Warren, H.S.
            </pre>

            <!--          Roura, S. (2001). A New Method for Balancing Binary Search Trees. I-->
            <!--          Chan, T.M. (2002). Closest-point problems simplified on the RAM. SO-->
            <!--          Warren, H.S. (2002). Hacker's Delight. 2nd Edition, section 5-3.-->
        </section>



        <section id="cost-balance">
            <h3>Cost-optimized weight-balance</h3>
            <p>
                There is one other design candidate that does not follow the same pattern.
                Cho and Sahni introduced the idea of <em>cost optimized search trees</em> in 2000,
                where the total search cost of the tree can not be reduced by a rotation anywhere in the tree,
                even if some nodes are not strictly weight-balanced by the classic definition.
            </p>
            <p>
                Even though their definition considers rotations specifically, we can apply this concept to partition-based balancing without the need to consider rotations at all.
                In this context, the only determination to make is whether a node is balanced or not, with no concern for how to restore that balance locally, given that the algorithm uses median-partitioning generally.
                To determine whether a node is cost-optimized by weight, we need to compare two levels of subtree sizes.
            </p>

            <p class="definition">
                <span class="underline">Definition:</span><br>
                <span class="indent">A node is cost-optimized if the size of each subtree is greater than or equal to the size of both subtrees of the other subtree.</span>
            </p>

            <pre>
    Cost() := (sl ≥ srl && sl ≥ srr) &&
              (sr ≥ sll && sr ≥ slr)
            </pre>

        </section>

        <section id="balancer-analysis">
            <h3>Analysis</h3>
            <p>
                10,000 trees were created in size increments of 10.
                At every step, a tree is grown, uniformly randomized, and balanced.
                All measurements should match exactly across systems because they measure the structure of the balanced tree, which is consistent given the same source of randomness.
            </p>
            <p>
                The results were generated on an Intel i5 13600K with 32GB of DDR5 memory.
            </p>



<!--            <div class="graphs">-->
<!--                <img src="benchmarks/svg/balancers/Partition/Uniform/PartitionCount__sbezier.svg">-->
<!--                <img src="benchmarks/svg/balancers/Partition/Uniform/TotalPartitionDepth__sbezier.svg">-->
<!--                <img src="benchmarks/svg/balancers/Partition/Uniform/AveragePartitionDepth__sbezier.svg">-->
<!--                <img src="benchmarks/svg/balancers/Partition/Uniform/Duration__sbezier.svg">-->
<!--                <img src="benchmarks/svg/balancers/Partition/Uniform/AveragePathLength__sbezier.svg">-->
<!--                <img src="benchmarks/svg/balancers/Partition/Uniform/MaximumPathLength__sbezier.svg">-->
<!--            </div>-->


            <section id="partition-count">
                <h4>Partition count</h4>

                <p>
                    Partition count is the number of times the balancing algorithm decided to partition a node.
                    This number is simply the total number of calls to <em>partition</em> to balance a random tree of some size.
                    Dividing this number by the size of the tree at each step gives us the average partition count per node.
                    A straight horizontal line indicates that partition count increases linearly in the size of the tree.
                </p>
                <p>
                    The results show that <mark class="red">height-balance partitions more nodes than median-balance, which is unexpected</mark>.
                    The entire idea behind height-balance as an alternative to strict median-balance was to avoid redundant partitioning.
                </p>
                <p>
                    On the other hand, the constant and logarithmic weight-balance strategies appear to consistently partition linearly in the size of the tree with very little variance.
                    Cost-optimized balance is somewhere in-between, overall less than median-balance and height-balance but more than weight-balance.
                </p>
                <p>
                    We can see clearly the effect of weight-balance relative to size-balance:
                    the overall partition count drops substantially by allowing branches to terminate with two nodes in the same direction, as expected.
                    Logarithmic weight-balance partitions the least often overall, though only slightly less often than constant weight-balance.
                </p>
                <div class="graphs">
                    <img src="benchmarks/svg/balancers/Partition/Uniform/PartitionCount__sbezier.svg">
                </div>
            </section>

            <section id="partition-depth">
                <h4>Partition depth</h4>
                <p>
                    Partition depth is the number of links that were followed during each partition, also the path length to each respective median, or simply <em>the amount of work done during partitioning</em>.
                </p>
                <p>
                    When we sum all partition depths and divide by the size of the tree, we get the average partition depth.
                    The shape of this plot matches what Muusse labels in Figure 5.3 of [] as <em>node visits divided by N</em>.
                </p>

                <div class="graphs">
                    <img src="benchmarks/svg/balancers/Partition/Uniform/TotalPartitionDepth__sbezier.svg">-->
                </div>
                <p>
                    A more direct evaluation of partition depth can be made by plotting partition depth against partition count, which is then the average depth per call to partition.
                    This is slightly different from plotting partition depth against the size of the tree because it normalizes instead against the partition count, which is different between strategies.
                </p>
                <p>
                    The results show that <mark class="yellow">height-balance consistently has the lowest average partition depth</mark>, which relieves some of the anxiety from the unexpected discovery that height-balance partitions more often than median-balance.
                    This presents an interesting competition of which is more efficient: to partition more often with shorter paths, or to partition less often with longer paths.
                </p>
                <div class="graphs">
                    <img src="benchmarks/svg/balancers/Partition/Uniform/AveragePartitionDepth__sbezier.svg">
                </div>

            </section>

            <section id="average-path-length">
                <h4>Average path length</h4>
                <p>
                    The total path length of a binary tree is the sum of the path length to every node from the root of the tree, and the average path length is that sum divided by the number of nodes.
                    This measurement can be thought of as the average depth, average search cost, or simply a measure of how balanced a tree is.
                </p>
                <p>
                    The results show that the average path length after balancing generally approaches log2(s).
                    Both median-balanced and height-balanced trees have the same minimal average path length, with cost-optimized trees only slightly worse.
                    Somewhere in the middle are size-balanced trees, followed by the weight-balanced trees.
                </p>
                <p>
                    Notice that the relative difference between them all is very small: the difference between the most-balanced and least-balanced result is ~3%.
                    At the final step where the size of the tree is 100,000 the average path length ranges between ~14.6 and ~15.1, which is not nothing but also not huge.
                </p>
                <p>
                    The conclusion is that all of these strategies produce well-balanced trees and proportionality is consistent.
                    As expected, we can see that the cost of using weight to allow branches to terminate with two nodes in the same direction has very little impact on average path length.
                </p>
                <div class="graphs">
                    <img src="benchmarks/svg/balancers/All/Uniform/AveragePathLength__sbezier.svg">
                </div>

            </section>

            <section id="maximum-path-length">
                <h4>Maximum path length</h4>
                <p>
                    The maximum path length (height bound) is the longest path from the root to any node, and can be thought of as the worst-case search cost.
                </p>
                <p>
                    The results are the same for median-balance and height-balance, as is the case with average path length.
                    We can see that the proportionality is consistent between all strategies, creeping up very slowly.
                    The order of best to worst is the same here as with average path length, with the two log-based weight-balanced strategies generally being the least-balanced.
                </p>
                <p>
                    However, keep in mind that this is the <em>worst-case</em> search cost, which is an unlikely branch to encounter often in most practical settings.
                </p>

                <div class="graphs">
                    <img src="benchmarks/svg/balancers/All/Uniform/MaximumPathLength__sbezier.svg">
                </div>
            </section>

            <section id="duration">
                <h4>Duration</h4>
                <p>
                    100 trees were also created in size increments of 10,000 to measure the amount of time it takes for each strategy to balance a randomized tree at each step.
                    Keep in mind however that duration-based benchmarks are system-dependent and the results could be entirely different on other machines.
                </p>
                <p>
                    Duration is often used by benchmarks as the one metric to rule them all because it practically evaluates the cost of memory interaction as well as the overhead of the algorithm itself.
                    There is often a structural measurement that correlates strongly with duration, but this is not always the case.
                </p>
                <p>
                    The results indicate a correlation between duration and partition count.
                    We can see that <mark class="red">median-balance is consistently faster than height-balance</mark>, which is unexpected because median-balance visits fewer nodes overall.
                    This suggests that the overhead of more calls to partition counts against height-balance enough to make it slower.
                </p>
                <p>
                    The performance of cost-optimized weight-balance seems to cut straight through median-balance, sometimes faster and sometimes slower, but always slower than the other weight-balanced strategies.
                </p>
                <p>
                    The size-balanced strategies are consistently ~20% faster than cost-optimized weight-balance, and <mark class="green">the weight-balanced strategies are ~30% faster</mark> respectively.
                </p>

                <div class="graphs">
                    <img src="benchmarks/svg/balancers/All/Uniform/Duration__sbezier.svg">
                </div>

                <p>
                    DSW is significantly slower than all partition-based strategies when the trees become large, but performs well when running the benchmarks on a smaller scale.
                    This is an interesting result since DSW is linear in time compared to partition-based balancing which is technically <a href="https://en.wikipedia.org/wiki/Time_complexity#Quasilinear_time">linearithmic</a>.
                </p>
                <p>
                    Even though DSW appears to be slower to balance large random trees, the algorithm has some clear advantages over partition-based balancing and should not be discounted from the outset.
                    A 2002 article by Timothy J. Rolfe brought attention back to the DSW algorithm, stating one advantage as being "pedagogically valuable within a course on data structures where one progresses from the binary search tree into self-adjusting trees, since it gives a first exposure to doing rotations within a binary search tree."
                    The two phases of DSW are also useful procedures individually: (i) to convert any binary search tree into a linked list, and (ii) to convert a linked list into a balanced tree, both in linear time and constant space.
                    Another consideration is that DSW does not require awareness of subtree size information which may not be available in some cases.
                </p>
            </section>

            <section id="balancer-conclusions">
                <h4>Conclusions</h4>
                <p>
                    We can safely say that all strategies explored here are viable in practice to balance a random binary search tree.
                    Generally, balancing by partition is an excellent strategy when trees are already at least somewhat-balanced.
                    The weight-based strategies are the fastest and incur only a slight relative increase in average path length, which is likely to be a good trade in practice.
                    The size-based strategies do more work than their weight-based counterparts but achieve slightly better balance.
                    Surprisingly, the benchmarks suggest that <mark>median-balance is the best choice for random trees when strict height-balance is a requirement</mark>.
                    DSW is the best choice to balance trees that are likely to be linear lists, or when subtree size information is not available.
                </p>
            </section>
        </section>
    </section>




    <section id="strategies-to-maintain-balance">
        <h2><span>Part 3</span>Self-Balancing Search Trees</h2>

        <p>
            Restoring balance to an entire tree is powerful but often disproportionately expensive because the entire tree must be traversed.
            There is also the question of <em>when</em> to balance to avoid balancing too often or to not balance often enough.
            Instead of either balancing the entire tree or not not balance at all, we can restore balance incrementally and thereby spread the balancing costs across updates.
            For example, an algorithm can make structural adjustments along the search path during an update to restore balance locally.
            A valid balancing algorithm does not change the ordering of the nodes and always returns the tree in a balanced state.
            A program can then <em>know</em> that a given tree is balanced because it is always balanced.
        </p>
        <p>
            This section explores various strategies to maintain balance over time and the algorithms to apply them.
            Some strategies make many structural changes while others are more selective; some maintain strict balance while others are entirely arbitrary.
            Our evaluation is to determine which strategies are likely to be a good choice in a particular context, generally considering the nature of balance and the work done to maintain it.
        </p>

        <section id="height-balanced-trees">
            <h3>Height-balanced trees</h3>
            <p>
                The original self-balancing binary search tree was published in 1962 as <em>an algorithm for the organization of information</em> by <strong>A</strong>del'son-<strong>V</strong>el'skii and <strong>L</strong>andis.
                <a href="https://en.wikipedia.org/wiki/AVL_tree">AVL trees</a> store the height of each node as part of the node structure and uses rotations based on these heights to maintain height-balance.
                Balancing after an insertion requires at most 2 rotations, but deletion may require a rotation at every node along the search path.
            </p>
            <p>
                All operations on an AVL tree start from the root, descend down the tree, then ascend back up to the root along the search path.
                Concurrency is therefore not viable because another operation would need to wait for the current operation to return to the root to potentially adjust it in some way.
                Generally, in an AVL tree only one operation can be performed at a time due to the recursive bottom-up nature of its algorithms.
            </p>
            <p>
                An AVL tree is always height-balanced, so the maximum path length is always logarithmic in the size of the tree.
                Recall that a persistent update requires that we copy all paths that lead to a modification.
                Since all structural adjustments on an AVL tree are local to the search path, a persistent update would need to copy all nodes along the search path.
                Persistence is therefore well-supported by AVL trees because a persistent update would only need to copy a logarithmic number of nodes.
            </p>
            <p>
                Since the invention of AVL trees, many self-balancing strategies have been proposed, the most common being the <a href="https://en.wikipedia.org/wiki/Red%E2%80%93black_tree">red-black tree</a> of Guibas and Sedgewick in 1978.
                Instead of directly storing the height of every node as in AVL trees, red-black trees use a color field in the node structure to track whether a node is <span class="rb-red">red</span> or <span class="rb-black">black</span>.
                The color red was chosen because it was the best-looking color produced by the laser printer available to the authors at the time, and they also used red and black pens to draw trees by hand.
                A set of rules based on these colors are used determine where to make rotations during an update to maintain balance: (i) a red node cannot link to other red nodes, and every path must have the same number of black nodes.
            </p>

            <p>
                Inserting into a red-black tree requires at most 2 rotations and deletion at most 3.
                The primary advantage of red-black trees is that the balancing overhead per operation is constant, i.e. not proportional to the size of the tree as is the case with deletion in AVL trees.
            </p>
            <p>
                Red-black trees are not strictly height-balanced because the heights of two subtrees can differ by more than 1 in some cases.
                The color-based rules do however impose an upper-bound on the height of a red-black tree, which is at most twice the log<span class="subscript">2</span> of its size.
            </p>
            <p>
                Compared to red-black trees, AVL trees are more balanced but rotate more often to maintain that balance.
                A <a href="https://stackoverflow.com/questions/13852870/red-black-tree-over-avl-tree/23277366#23277366">common response</a> to the recurring question of <span class="quote">should I use an AVL tree or a red-black tree</span> is to use an AVL tree when mostly searching because the average path length is lower, but to use red-black trees when mostly inserting and deleting because they require fewer rotations.
                This is however a misconception, because inserting and deleting both include searching, which is affected by path length all the same.
                There is a trade-off between the cost of doing more rotations and the cost of traversing longer paths.
<!--                The benchmarks confirm this, showing that <mark class="yellow">AVL trees are generally faster than an red-black trees</mark> even though they require more rotations on average.-->
<!--                When inserting and deleting in cycles, the path length of red-black trees slowly gets worse over time which correlates with a decrease in performance.-->
            </p>

<!--            <div class="graphs">-->
<!--                <img src="benchmarks/svg/operations/Insert/AVLRedBlack/Duration__sbezier.svg">-->
<!--                <img src="benchmarks/svg/operations/Insert/AVLRedBlack/Rotations__sbezier.svg">-->

<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLRedBlack/Duration__unique.svg">-->
<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLRedBlack/Rotations__unique.svg">-->

<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLRedBlack/AveragePathLength__unique.svg">-->
<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLRedBlack/MaximumPathLength__unique.svg">-->
<!--            </div>-->

            <p>
                Red-black trees are very common in lecture notes and in practice.
                Many of the C++ standard library <span class="code">set</span> implementations use red-black trees, as well as Java's <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/TreeMap.html#:~:text=A-,Red%2DBlack%20tree,-based%20NavigableMap%20implementation">TreeMap</a>, and parts of the <a href="https://github.com/torvalds/linux/blob/master/include/linux/rbtree.h">Linux kernel</a>.
                They are generally considered to be a decent all-rounder when it comes to self-balancing binary search trees, but attempting to implement them often results in frustration.
                At various stages throughout this project, especially when implementing the deletion algorithm without parent pointers, there was a sense of <span class="quote">this can't be it</span>.
            </p>
            <p>
                In 2013, Haeupler, Sen & Tarjan introduced <a href="https://en.wikipedia.org/wiki/WAVL_tree">rank-balanced trees</a> which unify many height-based balanced trees under a single framework.
                Rank-balanced trees store in each node an integer rank and use various rank-based rules to yield simple implementations of AVL trees, red-black trees, and others.
                A new strategy emerged from this framework: the <em>weak AVL tree</em>, which effectively combines the insertion of AVL trees with the deletion of red-black trees such that <mark class="yellow">the height bound degrades gracefully from that of an AVL tree as the number of deletions increase, and is never worse than that of a red-black tree</mark>.
                Insertion and deletion in a weak AVL tree both require at most 2 rotations, which is fewer than AVL trees and red-black trees.
            </p>
            <p>
                Unlike AVL trees, concurrency is well-supported by weak AVL trees because both <mark class="yellow">insertion and deletion can be implemented purely top-down</mark> with no need to return back to the root.
                The top-down algorithms are more complex and result in slightly different trees in some cases, but nonetheless produce valid weak AVL trees.
            </p>

<!--            <p>-->
<!--                Benchmarks show that weak AVL trees are faster than both AVL trees and red-black trees when only inserting, which is interesting because weak AVL trees have the exact same structure as AVL trees when no deletions occur.-->
<!--                Notice that the number of rotations when inserting is exactly the same between bottom-up AVL trees and bottom-up weak AVL trees.-->
<!--                This suggests that the weak AVL insertion algorithm itself is more efficient, likely due to fewer interactions with memory along the way.-->
<!--                Going through cycles of inserting and deleting show that weak AVL trees are consistently faster than AVL trees and maintain an average path length only slightly higher than AVL trees.-->
<!--                Notice how slowly the average path length worsens compared to red-black trees.-->
<!--            </p>-->


<!--            <div class="graphs">-->
<!--                <img src="benchmarks/svg/operations/Insert/AVLWeakRedBlack/Duration__sbezier.svg">-->
<!--                <img src="benchmarks/svg/operations/Insert/AVLWeakRedBlack/Rotations__sbezier.svg">-->

<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLWeakRedBlack/Duration__sbezier.svg">-->
<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLWeakRedBlack/Rotations__sbezier.svg">-->

<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLWeakRedBlack/AveragePathLength__sbezier.svg">-->
<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLWeakRedBlack/MaximumPathLength__sbezier.svg">-->
<!--            </div>-->

<!--            -->
<!--            <div class="graphs">-->
<!--                <img src="benchmarks/svg/operations/Insert/AVLWeak/Duration__sbezier.svg">-->
<!--                <img src="benchmarks/svg/operations/Insert/AVLWeak/Rotations__sbezier.svg">-->

<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLWeak/Duration__sbezier.svg">-->
<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLWeak/Rotations__sbezier.svg">-->

<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLWeak/AveragePathLength__sbezier.svg">-->
<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLWeak/MaximumPathLength__sbezier.svg">-->
<!--            </div>-->


            <p>
                Following on from this work in 2018, Sen, Tarjan & Kim introduced <a href="http://sidsen.azurewebsites.net//papers/ravl-trees-journal.pdf">relaxed rank-balanced trees</a> in which balancing is only done after insertion, not deletion.
                The balance of relaxed rank-balanced trees improves as nodes are inserted, and degrades very slowly as nodes are deleted.
                A subtle difference is that the height bound of a relaxed rank-balanced tree is proportional not to the size of the tree, which is often the case, but instead proportional to the number of insertions specifically.
                Relaxed variants of AVL trees and red-black trees are described in their paper and implemented here, both bottom-up and purely top-down.
            </p>

<!--            <div class="graphs">-->
<!--                <img src="benchmarks/svg/operations/Insert/AVLRelaxed/Duration__sbezier.svg">-->
<!--                <img src="benchmarks/svg/operations/Insert/AVLRelaxed/Rotations__sbezier.svg">-->

<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLRelaxed/Duration__sbezier.svg">-->
<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLRelaxed/Rotations__sbezier.svg">-->

<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLRelaxed/AveragePathLength__sbezier.svg">-->
<!--                <img src="benchmarks/svg/operations/InsertDeleteCycles/AVLRelaxed/MaximumPathLength__sbezier.svg">-->
<!--            </div>-->

            <p>
                On a completely different track, Prokopec and Odersky presented <a href="https://en.wikipedia.org/wiki/Conc-tree_list">conc-tree lists</a> in 2017, where nodes either have two subtrees or no subtrees.
                Nodes with subtrees are used only for routing purposes, and nodes without subtrees contain the values of the sequence.
                Additionally, like in AVL trees, every node stores its height and the height difference between two nodes differ by no more than 1.
            </p>
            <pre>
        <>          <>  =  routing node, no value
      /    \        ()  =  no subtrees, has value
     <>    (c)
    /  \
  (a)  (b)
            </pre>
            <p>
                Conc-trees are very simple to implement and do not use rotations or explicit copying.
                Instead, they make use of functional composition and smart constructors, where rather than copying and modifying an existing node it allocates and composes a new node by combining properties of other nodes.
                Conceptually, conc-trees <em>construct</em> the resulting path more than they modify the existing one.
            </p>

<!--            <div class="graphs">-->
<!--                <img src="benchmarks/svg/operations/InsertPersistent/AVLConc/Duration__sbezier.svg">-->

<!--                <img src="benchmarks/svg/operations/InsertDeleteCyclesPersistent/AVLConc/Duration__sbezier.svg">-->

<!--                <img src="benchmarks/svg/operations/InsertDeleteCyclesPersistent/AVLConc/AveragePathLength__sbezier.svg">-->
<!--                <img src="benchmarks/svg/operations/InsertDeleteCyclesPersistent/AVLConc/MaximumPathLength__sbezier.svg">-->
<!--            </div>-->

        </section>

        <section id="weight-balanced-trees">
            <h3>Weight-balanced trees</h3>

            <p>
                Implementing a linear list using a binary search tree requires that we track the size of each subtree along the search path.
                It seems ideal then to use this information for balancing, rather than heights or ranks stored additionally in each node.
                The original paper by Nievergelt & Reingold in 1972 introduced weight-balanced trees as <em>trees of bounded balance</em> or BB[<span class="math">α</span>] trees, where <span class="math">0 ≤ α ≤ ½</span> is a parameter to trade-off between better balance and fewer rotations.
                Many other papers on weight-balanced trees since then use two parameters: <span class="math">Δ</span> to determine if a node is weight-balanced, and <span class="math">Γ</span> to determine whether one or two rotations are required to restore balance.
                The valid range for these parameters are complex, involving either fractional arithmetic or multiplication with the potential for integer overflow.
            </p>

            <section id="logarithmic-weight-balanced-trees">
                <h4>Logarithmic weight-balanced trees</h4>
                <p>
                    The weight-balance strategy suggested by Roura in 2001 use simple bitwise operations to determine logarithmic weight-balance, avoiding both fractional arithmetic and integer overflow.
                    The height bound of a logarithmic weight-balanced tree is the same as a red-black tree, no greater than <span class="math">2*⌊log₂(s)⌋</span>.
                    Leonor Frias implemented logarithmic weight-balanced trees top-down in 2005 [], which maintain balance in constant space using only a single top-down pass for both insertion and deletion.
                    Hirai & Yamamoto published a rigorous investigation of weight-balanced trees in 2011, which included a short section on logarithmic weight-balance stating that <span class="quote">for mathematical reliability, logarithmic weight-balanced trees are simpler</span>.
                    They measured the performance of logarithmic weight-balanced trees to be on-par with parametric weight-balance, but used the original recursive bottom-up implementation suggested by Roura rather than the top-down variant developed by Frias.
                    Barth & Wagner published <em>engineering top-down weight-balanced trees</em> in 2019, wherein they evaluate viable parameters to implement weight-balanced trees purely top-down, but mention logarithmic weight-balance only in the introduction without further evaluation.
                </p>
                <p>
                    The involvement of the binary log in the context of binary trees seems more natural than a complex set of balancing parameters, especially since the structural properties and measured performance appears to be equivalent.
                    There appears to be no practical reason then to use parametric weight-balance over logarithmic weight-balance, to which we focus our attention.
                    This project includes two simple implementations of logarithmic weight-balanced trees: <span class="code">LBSTBottomUp</span> and <span class="code">LBSTTopDown</span>.
                </p>
                <p>
                    The bottom-up variant is the original recursive implementation by Roura, but uses a slightly simpler bitwise expression to determine balance: <span class="code">x < ^x & y</span> instead of <span class="code">x < y && ((x&y) << 1) < y</span>.
                    The top-down variant is based on the general weight-balancing algorithm of Barth & Wagner that uses subtree weight instead of subtree size; simply <span class="math">size+1</span> instead of <span class="math">size</span>.
                    Using weight instead of size for the top-down approach avoids a tricky double-rotate case during insertion while preserving the height bound of size-based variant, at the cost of a slightly higher average search cost.
                </p>
            </section>

            <section id="relaxed-weight-balanced-trees">
                <h4>Relaxed logarithmic weight-balanced trees</h4>

                <p>
                    Exploring this space further we find the idea of <em>partial rebuilding</em> by Overmars in 1983. TODO
                    Further development produced the <a href="">general balanced trees</a> of Andersson in 1989 and 1999, followed by the <a href="">scapegoat tree</a> of Galperin & Rivest in 1993.
                </p>
                <p>
                    A weight-balanced tree has a known maximum height bound given that every node of the tree is weight balanced, by definition.
                    For example, when all the nodes of a binary search tree are logarithmically weight-balanced its height can not be greater than <span class="math">2*⌊log₂(s)⌋</span>.
                    Therefore, when the height of a tree exceeds this bound there must be at least one node that is <em>not</em> weight-balanced.
                    Galperin & Rivest refer to this node as the <span class="quote">scapegoat</span>.
                    The basic idea of partial rebuilding is to find this node and restore its balance, thereby restoring the height-bound of the tree.
                    Intuitively, the <mark>height of a node that was <em>not</em> balanced decreases when it becomes balanced</mark>.
                    This strategy therefore combines the ideas of weight-balance and height-balance by allowing some nodes to not be weight-balanced as long as the overall height-bound is valid.
                </p>
                <pre>

[ image of a tree that has a perfect left subtree
  of 15 nodes, a root, and 1 node in its right subtree,
  showing that the root is obviously not weight-balanced
  but that the tree has a low average search cost ]

                </pre>
                <p>
                    A scapegoat tree uses a parameter <span class="math">½ ≤ α < 1</span> and defines that <span class="math">height ≤ ⌊log<span class="subscript">1/α</span>(s)⌋+1</span> and that a node is <span class="math">α</span>-weight-balanced if <span class="math">sl ≤ α * s)</span> and <span class="math">sr ≤ α * s)</span>.
                    The insertion algorithm as presented in [] is a two-phase recursive algorithm, (i) search by descending from the root and attach a new node, then (ii) ascend bottom-up to find a scapegoat that is not <span class="math">α</span>-weight-balanced.
                    There must be at least one node along the search path that is not balanced when the insertion depth exceeds the height bound, so the second phase checks for <span class="math">α</span>-weight-balance until a scapegoat is found, then rebuilds that entire subtree to restore the height bound.
                    The primary rebuilding algorithm suggested in [] is similar to Day-Stout-Warren, recursively flattening the tree into a list then compressing the list back into a perfectly-balanced tree.
                </p>

                <p>
                    There are a few downsides to this strategy as presented:
                    (i) the recursion must unwind all the way back to the root even when the insertion did not exceed the height bound,
                    (ii) the <span class="math">α</span>-parameter imposes an awkward fractional log base and may require floating point arithmetic to determine weight-balance,
                    (iii) the rebuilding algorithm does not make use of the fact that the subtree is likely already somewhat-balanced.
                </p>
                <p>
                    Recursion can be avoiding by optimistically looking for a scapegoat top-down before the insertion depth is determined.
                    The scapegoat is then already in-hand when the height bound is exceeded after a new node is attached at the bottom of the tree.
                    Checking weight-balance optimistically on the way down might be redundant because the result may not be relevant in the end, but conventional weight-balanced trees already evaluate balance at every node anyway so this is okay.
                    Consider that there are potentially multiple nodes along the search path that qualify as a scapegoat, offering a choice: use the first one encountered higher up in the tree, or the last one encountered lower in the tree.
                    The logical implication is simple: a preference for the highest node skips further weight-balance checks if a scapegoat has been identified, otherwise keep checking and overwrite the existing reference.
                    Using the lowest node is a simple choice when already using the recursive approach, also because once you pass by one there is no guarantee that another exists higher up still.
                    There are however some hints that preferring a node higher up in the tree is likely to produce better results.
                    Overmars originally suggested using the highest node in [], and Galperin & Rivest write in [] that <span class="quote">in our experiments this heuristic performed better than choosing the first weight-unbalanced ancestor to be the scapegoat</span>.
                    Even so, every implementation or pedagogical reference currently available for scapegoat trees use the recursive method and prefer the deepest scapegoat.
                </p>

                <p>
                    The matter of the <span class="math">α</span>-parameter can be solved by using the logarithmic weight-balance definition of Roura in [].
                    Given that the height bound of a logarithmic weight-balanced tree is <span class="math">2*⌊log₂(s)⌋</span>, it follows that when the insertion depth exceeds this bound for the current size that there must be a node along the search path that was not logarithmically weight-balanced.
                    For a insertion depth <span class="math">d</span> and a tree of size <span class="math">s</span>, the height bound is exceeded when <span class="math">d > 2*⌊log₂(s)⌋</span> which can be evaluated using bitwise operations as <span class="code">(1 << ((d + 1) >> 1)) > s</span>.
                </p>

                <p>
                    The final sharp edge to resolve is the choice of algorithm to rebuild the subtree of the scapegoat.
                    We can assert that the subtree is already somewhat-balanced, and we know that restoring logarithmic weight-balance at every node in that subtree would restore the height bound.
                    Using the partition-based logarithmic weight-balance strategy from earlier in our exploration is therefore an excellent choice, operating both top-down and in constant time while making use of the fact that most nodes would not require partitioning.
                </p>

                <p>
                    <span class="quote">What about deletion?</span>
                </p>

                <p>
                    Alongside the reference to the root node and the current size of the tree, scapegoat trees also store a <em>maximum size</em> field, which is the maximum size that the tree has reached since the root was rebuilt.
                    The theory follows that the entire tree should be rebuilt when the current size becomes less than <span class="math">α</span> times the maximum size, then to reset the maximum size to the current size.
                    Occasionally rebuilding the tree when many deletions have occurred ensures that the height is always logarithmic in the current size of the tree.
                    Alternatively, we can use the concept of <em>relaxed balance</em> introduced by Sen, Tarjan & Kim to only balance after an insertion and not after a deletion.
                    The intuition here is that height can not increase when a node is deleted, and the next insertion that occurs will inevitably restore the height bound again, possibly at the root if that is the first encountered node that is not weight-balanced.
                    The height is then at most logarithmic in the number of insertions rather than the current size of the tree, but possibly more tightly proportional to the delta between insertions and deletions.
                    This approach avoids the need to maintain the maximum size field, and thereby yields a third logarithmic weight-balance strategy: <span class="code">LBSTRelaxed</span>.
                </p>

            </section>

        </section>

        <section id="randomly-balanced-trees">
            <h3>Randomly-balanced trees</h3>

        </section>

        <section id="self-adjusting-trees">
            <h3>Self-adjusting trees</h3>

        </section>

        <section id="join-based-trees">
            <h3>Join-based trees</h3>

        </section>


    </section>

    <section id="references">
        <h2>References</h2>
        <ol>
            <li></li>
        </ol>
    </section>

<!--    <pre>-->
<!--        1. Introduction-->
<!--            - Giving semantic meaning to abstract memory-->
<!--            - Linear lists-->
<!--                - List operations-->
<!--                - Dynamic arrays-->
<!--                - Linked lists-->
<!--            - Binary search trees-->
<!--                - Relative position-->
<!--                - Node structure-->
<!--                - Traversal-->
<!--                - Balance-->
<!--                - Rotations-->
<!--            - Persistence-->
<!--                - Structural sharing-->
<!--                - Immutability-->
<!--                - Parent pointers-->
<!--            - Concurrency-->
<!--                - Contention-->
<!--                - Recursion-->
<!--            - Benchmarking-->
<!--                - Access distributions-->
<!--                - Operations-->
<!--        2. Strategies to restore balance-->
<!--            - Balancing by rotation-->
<!--                - Day-Stout-Warren-->
<!--            - Balancing by recursive median partitioning-->
<!--                - Height balance-->
<!--                - Weight balance-->
<!--        3. Strategies to maintain balance-->
<!--            - Join-based implementations-->
<!--            - Rank-balanced trees-->
<!--            - Weight-balanced trees-->
<!--            - Randomly-balanced trees-->
<!--            - Self-adjusting trees-->
<!--            - Height-balanced trees-->
<!--        4. Evaluation-->
<!--            5. Measurements-->
<!--            6. Evaluation criteria-->
<!--            5. Benchmarks-->
<!--            7. Conclusions-->
<!--               Source code-->
<!--            9. Repository-->
<!--            10. Animations-->
<!--            11. Contributing-->
<!--                -. Open problems-->
<!--            - Missing proofs of correctness-->
<!--            - Complexity of balancing by median partitioning-->
<!--            - Similar analysis for set data structures-->
<!--            - Translation to another language, perhaps Rust-->
<!--            - Comparisons to other list data structures-->
<!--              Notes-->
<!--              References-->
<!--        </pre>-->
<!--    </section>-->
</article>
<!--https://read.seas.harvard.edu/~kohler/notes/llrb.html-->
<!--https://web.archive.org/web/20131106123552/http://t-t-travails.blogspot.com/2008/04/left-leaning-red-black-trees-are-hard.html-->
<!--https://web.archive.org/web/*/http://t-t-travails.blogspot.com/2008/04/left-leaning-red-black-trees-are-hard.html-->
<!--https://github.com/zarif98sjs/RedBlackTree-An-Intuitive-Approach-->
<!--https://www.boost.org/doc/libs/1_53_0/doc/html/intrusive/set_multiset.html-->
</body>
</html>


